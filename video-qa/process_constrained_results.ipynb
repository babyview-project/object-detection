{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f946a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641a5a1",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "- Separately get locations for bing vs home\n",
    "- Remove house, home\n",
    "- Return >1 activity per response (maybe look at probabilities)\n",
    "- Differentiate egocentric vs other activity\n",
    "- Check that the activities are not flickering back and forth\n",
    "- \"Other\" to scan through what is being missed maybe\n",
    "- Compare the unconstrained to SAYCam list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8354ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = '/ccn2/dataset/babyview/outputs_20250312/activities/videollama3_constrained'\n",
    "csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "print(f'Found {len(csv_files)} CSV files in {csv_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adac3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and concatenate all CSV files into a single DataFrame, then extract the Location and Activity series.\n",
    "dfs = []\n",
    "non_bing_counts = 0\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    # Skip Bing files\n",
    "    if csv_file.startswith('01') or csv_file.startswith('02'):\n",
    "        continue\n",
    "    non_bing_counts += 1\n",
    "    file_path = os.path.join(csv_dir, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Only include files with both columns\n",
    "    if 'Location' in df.columns and 'Activity' in df.columns:\n",
    "        dfs.append(df)\n",
    "        \n",
    "big_df = pd.concat(dfs, ignore_index=True)\n",
    "print('Non-Bing CSV files:', non_bing_counts)\n",
    "print(f'Combined DataFrame has {len(big_df)} rows.')\n",
    "\n",
    "# Extract and convert to strings\n",
    "location_series = big_df['Location'].astype(str)\n",
    "activity_series = big_df['Activity'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ae538",
   "metadata": {},
   "source": [
    "## Separately: Locations, Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6459a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_counts = location_series.value_counts()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(location_counts.index, location_counts.values, color='skyblue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Location\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Counts of Each Unique Location\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_counts = activity_series.value_counts()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(activity_counts.index, activity_counts.values, color='skyblue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Location\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Counts of Each Unique Activity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62557f4d",
   "metadata": {},
   "source": [
    "# Locations + Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa08813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = big_df.copy()\n",
    "df = df.dropna(subset=[\"Location\", \"Activity\"])\n",
    "for col in [\"Location\", \"Activity\"]:\n",
    "    df[col] = df[col].astype(str).str.strip().fillna(\"Unknown\")\n",
    "\n",
    "# contingency table\n",
    "ct = pd.crosstab(df[\"Location\"], df[\"Activity\"])\n",
    "\n",
    "# (optional) keep top-N by totals so the plot is readable\n",
    "top_rows = ct.sum(1).nlargest(20).index\n",
    "top_cols = ct.sum(0).nlargest(20).index\n",
    "ct = ct.loc[top_rows, top_cols]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(8, 0.55*len(ct.columns)), max(6, 0.45*len(ct))))\n",
    "im = ax.imshow(ct.values, aspect=\"auto\")\n",
    "ax.set_xticks(np.arange(ct.shape[1])); ax.set_xticklabels(ct.columns, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(np.arange(ct.shape[0])); ax.set_yticklabels(ct.index)\n",
    "ax.set_xlabel(\"Activity\"); ax.set_ylabel(\"Location\"); ax.set_title(\"Location × Activity (counts)\")\n",
    "plt.colorbar(im, ax=ax, label=\"Count\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46308d5",
   "metadata": {},
   "source": [
    "### 2) Row-normalized (“distribution of activities within each location”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_share = ct.div(ct.sum(1).replace(0, 1), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(8, 0.55*len(row_share.columns)), max(6, 0.45*len(row_share))))\n",
    "im = ax.imshow(row_share.values, aspect=\"auto\", vmin=0, vmax=1)\n",
    "ax.set_xticks(np.arange(row_share.shape[1])); ax.set_xticklabels(row_share.columns, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(np.arange(row_share.shape[0])); ax.set_yticklabels(row_share.index)\n",
    "ax.set_xlabel(\"Activity\"); ax.set_ylabel(\"Location\"); ax.set_title(\"Activity share within each Location\")\n",
    "plt.colorbar(im, ax=ax, label=\"Share (0–1)\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016262d3",
   "metadata": {},
   "source": [
    "### 3) “Surprise” map (which pairs are over/under-represented given totals)\n",
    "\n",
    "This controls for marginals and highlights associations, not just frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7eaee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "obs = ct.values\n",
    "row_tot = obs.sum(axis=1, keepdims=True)\n",
    "col_tot = obs.sum(axis=0, keepdims=True)\n",
    "grand = obs.sum()\n",
    "expected = row_tot @ col_tot / grand\n",
    "std_resid = (obs - expected) / np.sqrt(np.maximum(expected, 1e-9))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(8, 0.55*ct.shape[1]), max(6, 0.45*ct.shape[0])))\n",
    "im = ax.imshow(std_resid, aspect=\"auto\")\n",
    "ax.set_xticks(np.arange(ct.shape[1])); ax.set_xticklabels(ct.columns, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(np.arange(ct.shape[0])); ax.set_yticklabels(ct.index)\n",
    "ax.set_xlabel(\"Activity\"); ax.set_ylabel(\"Location\"); ax.set_title(\"Standardized residuals (obs − exp) / √exp\")\n",
    "plt.colorbar(im, ax=ax, label=\"Std residual (±)\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817266b5",
   "metadata": {},
   "source": [
    "### 4) Alternative view (stacked bars)\n",
    "\n",
    "Good when you have many activities but want per-location distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482550b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_share.plot(kind=\"bar\", stacked=True, figsize=(max(8, 0.7*len(row_share)), 6))\n",
    "plt.ylabel(\"Share\"); plt.title(\"Activities within each Location\")\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\"); plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6b50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c280e5c",
   "metadata": {},
   "source": [
    "## IMU + (Locations, Activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_imu_dir = '/ccn2/dataset/babyview/outputs_20250312/imu/10s_clips/'\n",
    "imu_csv_files = [f for f in os.listdir(overall_imu_dir) if f.endswith('.csv')]\n",
    "\n",
    "dfs_imu = []\n",
    "for f in tqdm(imu_csv_files, desc=\"Processing IMU CSV files\"):\n",
    "    df_imu = pd.read_csv(os.path.join(overall_imu_dir, f))\n",
    "    dfs_imu.append(df_imu)\n",
    "imu_df = pd.concat(dfs_imu, ignore_index=True)\n",
    "print(f\"Concatenated {len(imu_csv_files)} files into a DataFrame with {len(imu_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge big_df with imu_df on video_id, big_df should have the main key,\n",
    "# and drop the imu rows which did not match\n",
    "merged_df = big_df.merge(imu_df, on=\"video_id\", how=\"inner\", suffixes=(\"\", \"_imu\"))\n",
    "print('Merged DataFrame has', len(merged_df), 'rows after inner join on video_id.')\n",
    "\n",
    "merged_df = merged_df.dropna(how=\"any\")\n",
    "print('Merged DataFrame has', len(merged_df), 'rows after dropping rows with any NaN values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = merged_df.copy()\n",
    "\n",
    "acc_cols  = [\"ACCL_X (m/s²)\",\"ACCL_Y (m/s²)\",\"ACCL_Z (m/s²)\"]\n",
    "grav_cols = [\"GRAV_X (m/s²)\",\"GRAV_Y (m/s²)\",\"GRAV_Z (m/s²)\"]\n",
    "\n",
    "df[acc_cols+grav_cols] = df[acc_cols+grav_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "A = df[acc_cols].to_numpy()\n",
    "G = df[grav_cols].to_numpy()\n",
    "\n",
    "# Detect GRAV_* scale (≈1 or ≈9.81). Your screenshot suggests ≈1.\n",
    "g_norm = np.nanmedian(np.linalg.norm(G, axis=1))\n",
    "if 0.5 < g_norm < 2:        # GRAV_* is ~unit vector: scale to m/s²\n",
    "    grav_scale = 9.81\n",
    "elif 8 < g_norm < 11:       # already in m/s²\n",
    "    grav_scale = 1.0\n",
    "else:                        # fallback: rescale so ||G|| ≈ 9.81\n",
    "    grav_scale = 9.81 / g_norm\n",
    "\n",
    "G_mps2 = G * grav_scale\n",
    "\n",
    "# Linear (gravity-removed) acceleration per axis\n",
    "LIN = A - G_mps2\n",
    "df[\"LIN_X (m/s²)\"], df[\"LIN_Y (m/s²)\"], df[\"LIN_Z (m/s²)\"] = LIN.T\n",
    "\n",
    "# Magnitudes and decomposition relative to gravity\n",
    "lin_norm = np.linalg.norm(LIN, axis=1)\n",
    "g_hat = G_mps2 / (np.linalg.norm(G_mps2, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "df[\"lin_norm\"] = lin_norm                                   # orientation-invariant intensity\n",
    "df[\"lin_parallel_g\"]  = (LIN * g_hat).sum(1)               # up/down component\n",
    "df[\"lin_perp_g_norm\"] = np.linalg.norm(LIN - df[\"lin_parallel_g\"].to_numpy()[:,None]*g_hat, axis=1)  # horizontal\n",
    "# Tilt (0° = upright). In your system, upright has ACCL_Y ≈ -9.8 ⇒ gravity points toward -Y.\n",
    "df[\"tilt_deg\"] = np.degrees(np.arccos(np.clip(np.abs(-g_hat[:,1]), 0, 1)))\n",
    "\n",
    "# --- Gyroscope features (rotational motion) ---\n",
    "gyro_cols = [\"GYRO_X (rad/s)\", \"GYRO_Y (rad/s)\", \"GYRO_Z (rad/s)\"]\n",
    "df[gyro_cols] = df[gyro_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "W = df[gyro_cols].to_numpy()\n",
    "\n",
    "# Total rotational speed\n",
    "df[\"gyro_norm\"] = np.linalg.norm(W, axis=1)  # rad/s\n",
    "\n",
    "# Decompose relative to gravity direction g_hat:\n",
    "#   parallel  ~ yaw rate (rotation around vertical/gravity axis)\n",
    "#   perpendicular magnitude ~ pitch/roll rate combined\n",
    "gyro_parallel = (W * g_hat).sum(axis=1)                       # signed yaw rate (rad/s)\n",
    "gyro_perp     = W - gyro_parallel[:, None] * g_hat\n",
    "df[\"gyro_parallel_g\"]   = gyro_parallel                       # rad/s\n",
    "df[\"gyro_perp_g_norm\"]  = np.linalg.norm(gyro_perp, axis=1)   # rad/s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cb0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_by_signed(data: pd.DataFrame, cat: str, value: str, *,\n",
    "                     top:int=20, lower_q:float=0.005, upper_q:float=0.995,\n",
    "                     order_by:str=\"abs_median\", save:bool=False):\n",
    "    \"\"\"\n",
    "    Make a single violin plot of a signed IMU component grouped by a category.\n",
    "    order_by: 'abs_median' (magnitude-focused) or 'median' (signed).\n",
    "    \"\"\"\n",
    "    tmp = data.dropna(subset=[cat, value]).copy()\n",
    "\n",
    "    # Keep top categories by frequency so the figure stays readable\n",
    "    keep = tmp[cat].value_counts().nlargest(top).index\n",
    "    tmp = tmp[tmp[cat].isin(keep)]\n",
    "\n",
    "    # Winsorize tails symmetrically (preserve sign, just trim extremes)\n",
    "    q_lo = tmp[value].quantile(lower_q)\n",
    "    q_hi = tmp[value].quantile(upper_q)\n",
    "    tmp[value] = tmp[value].clip(q_lo, q_hi)\n",
    "\n",
    "    # Choose ordering\n",
    "    if order_by == \"abs_median\":\n",
    "        order = (tmp.groupby(cat)[value]\n",
    "                   .apply(lambda s: s.abs().median())\n",
    "                   .sort_values(ascending=False).index.tolist())\n",
    "    else:  # 'median'\n",
    "        order = (tmp.groupby(cat)[value].median()\n",
    "                   .sort_values(ascending=False).index.tolist())\n",
    "\n",
    "    # Prepare data arrays\n",
    "    data_arrays = [tmp.loc[tmp[cat] == c, value].to_numpy() for c in order]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(max(8, 0.55*len(order)), 5))\n",
    "    _ = ax.violinplot(data_arrays, showextrema=False)\n",
    "\n",
    "    # Overlay robust summary: median (•) and IQR (┃)\n",
    "    g = tmp.groupby(cat)[value]\n",
    "    med = g.median().reindex(order).to_numpy()\n",
    "    q1  = g.quantile(0.25).reindex(order).to_numpy()\n",
    "    q3  = g.quantile(0.75).reindex(order).to_numpy()\n",
    "    x   = np.arange(1, len(order)+1)\n",
    "    ax.scatter(x, med, s=18, zorder=3)\n",
    "    ax.vlines(x, q1, q3, linewidth=3, alpha=0.9, zorder=2)\n",
    "\n",
    "    ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.7)  # zero reference\n",
    "    ax.set_xticks(x); ax.set_xticklabels(order, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(value)\n",
    "    ax.set_title(f\"{value} by {cat} (per row)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        safe_value = value.replace(\"/\", \"_per_\").replace(\" \", \"_\")\n",
    "        plt.savefig(f\"{safe_value}_by_{cat}.png\", dpi=150)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496afa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_signed_cols = [\"LIN_X (m/s²)\",\"LIN_Y (m/s²)\",\"LIN_Z (m/s²)\",\n",
    "                   \"GYRO_X (rad/s)\",\"GYRO_Y (rad/s)\",\"GYRO_Z (rad/s)\"]\n",
    "\n",
    "for col in [\"gyro_norm\", \"lin_norm\",\"lin_parallel_g\",\"lin_perp_g_norm\", \"tilt_deg\"]:\n",
    "    violin_by_signed(df, \"Activity\", col, top=20, order_by=\"median\")\n",
    "    violin_by_signed(df, \"Location\", col, top=20, order_by=\"median\")\n",
    "    \n",
    "for col in imu_signed_cols:\n",
    "    violin_by_signed(df, \"Activity\", col, top=20, order_by=\"abs_median\")\n",
    "    violin_by_signed(df, \"Location\", col, top=20, order_by=\"abs_median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb77d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccwm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
