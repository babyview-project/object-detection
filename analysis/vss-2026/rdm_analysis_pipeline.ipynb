{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VSS 2026 RDM Analysis Pipeline\n",
    "\n",
    "This notebook provides a complete, reproducible pipeline for computing and analyzing Representational Dissimilarity Matrices (RDMs) for CLIP and DINOv3 category embeddings.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Compute RDM Matrices** - Compute pairwise RDM matrices from category embeddings\n",
    "2. **Filter and Reorganize RDMs** (Optional) - Filter out low-quality categories and reorganize by type\n",
    "3. **Correlate RDM Matrices** - Compare RDM matrices between different models\n",
    "4. **Correlate Category Embeddings** (Optional) - Compare category-level embeddings directly\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have the following packages installed:\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `scipy`\n",
    "- `matplotlib`\n",
    "- `seaborn`\n",
    "- `scikit-learn`\n",
    "- `tqdm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, optimal_leaf_ordering\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib backend\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Please update the paths below according to your setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR SETUP\n",
    "# ============================================================================\n",
    "#\n",
    "# IMPORTANT: If you already have average embeddings saved as .npz files,\n",
    "# you can skip Step 1 and start directly from Step 2! Just set CLIP_OUTPUT_DIR\n",
    "# (or DINOV3_OUTPUT_DIR) to point to the directory containing your \n",
    "# category_average_embeddings.npz file.\n",
    "#\n",
    "# Example: If your file is at \"./clip_rdm_results_26/category_average_embeddings.npz\",\n",
    "# set CLIP_OUTPUT_DIR = \"./clip_rdm_results_26\"\n",
    "# ============================================================================\n",
    "\n",
    "# Paths for CLIP embeddings\n",
    "# For Step 1: Set these to compute RDMs from individual embeddings\n",
    "CLIP_EMBEDDING_LIST = None  # Path to text file with CLIP embedding paths (one per line), or None to scan directory\n",
    "CLIP_EMBEDDINGS_DIR = \"/path/to/clip_embeddings\"  # Base directory for CLIP embeddings (only needed for Step 1)\n",
    "\n",
    "# For Step 2+: Point this to directory containing category_average_embeddings.npz\n",
    "# Example: \"./clip_rdm_results_26\" if your .npz file is in that directory\n",
    "CLIP_OUTPUT_DIR = \"./clip_rdm_results\"  # Directory containing category_average_embeddings.npz (or output dir for Step 1)\n",
    "\n",
    "# Paths for DINOv3 embeddings (optional)\n",
    "# For Step 1: Set these to compute RDMs from individual embeddings\n",
    "DINOV3_EMBEDDING_LIST = None  # Path to text file with DINOv3 embedding paths, or None to scan directory\n",
    "DINOV3_EMBEDDINGS_DIR = \"/path/to/dinov3_embeddings\"  # Base directory for DINOv3 embeddings (only needed for Step 1)\n",
    "\n",
    "# For Step 2+: Point this to directory containing category_average_embeddings.npz\n",
    "# Example: \"./dinov3_rdm_results_26\" if your .npz file is in that directory\n",
    "DINOV3_OUTPUT_DIR = \"./dinov3_rdm_results\"  # Directory containing category_average_embeddings.npz (or output dir for Step 1)\n",
    "MATCH_FROM_CLIP_LIST = True  # If True, match DINOv3 filenames from CLIP list (ensures same images)\n",
    "\n",
    "# CDI words CSV file (required for category type organization in Step 2)\n",
    "CDI_PATH = \"./data/cdi_words.csv\"\n",
    "\n",
    "# Filtering options (optional, used in Step 2)\n",
    "EXCLUSION_FILE = None  # Path to text file with categories to exclude (one per line), or None\n",
    "INCLUSION_FILE = None  # Path to text file with categories to include (one per line), or None\n",
    "FILTERED_OUTPUT_DIR = \"./clip_rdm_results_filtered\"  # Output directory for filtered results\n",
    "\n",
    "# Processing options\n",
    "NUM_WORKERS = None  # Number of parallel workers (None = auto-detect, max 16)\n",
    "USE_PARALLEL = True  # Enable parallel loading (only used in Step 1)\n",
    "USE_CLUSTERING = True  # Enable hierarchical clustering within category groups (used in Step 2)\n",
    "SAVE_DENDROGRAMS = False  # Save dendrogram plots for each category group (used in Step 2)\n",
    "\n",
    "# Correlation options\n",
    "CORRELATE_RDMS = False  # Set to True to correlate RDM matrices (requires both CLIP and DINOv3 results)\n",
    "CORRELATE_EMBEDDINGS = False  # Set to True to correlate category embeddings\n",
    "\n",
    "print(\"Configuration loaded. Please review and update paths as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These functions are used throughout the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_paths(txt_path):\n",
    "    \"\"\"Load embedding file paths from text file\"\"\"\n",
    "    print(f\"Loading embedding paths from {txt_path}...\")\n",
    "    with open(txt_path, 'r') as f:\n",
    "        paths = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Found {len(paths)} embedding paths\")\n",
    "    return paths\n",
    "\n",
    "def scan_embedding_directory(embeddings_dir):\n",
    "    \"\"\"Scan directory for all .npy embedding files\"\"\"\n",
    "    embeddings_dir = Path(embeddings_dir)\n",
    "    print(f\"Scanning {embeddings_dir} for .npy files...\")\n",
    "    \n",
    "    npy_files = list(embeddings_dir.rglob(\"*.npy\"))\n",
    "    \n",
    "    if len(npy_files) == 0:\n",
    "        raise ValueError(f\"No .npy files found in {embeddings_dir}\")\n",
    "    \n",
    "    paths = [str(f.relative_to(embeddings_dir)) for f in npy_files]\n",
    "    paths.sort()\n",
    "    \n",
    "    print(f\"Found {len(paths)} embedding files\")\n",
    "    return paths\n",
    "\n",
    "def match_embedding_paths_from_list(reference_list_path, target_embeddings_dir):\n",
    "    \"\"\"Match embedding paths from a reference list to target directory\"\"\"\n",
    "    target_embeddings_dir = Path(target_embeddings_dir)\n",
    "    \n",
    "    print(f\"Loading reference embedding list from {reference_list_path}...\")\n",
    "    with open(reference_list_path, 'r') as f:\n",
    "        reference_paths = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Found {len(reference_paths)} reference paths\")\n",
    "    \n",
    "    reference_mapping = {}\n",
    "    for ref_path in reference_paths:\n",
    "        ref_path_obj = Path(ref_path)\n",
    "        if len(ref_path_obj.parts) >= 2:\n",
    "            category = ref_path_obj.parts[-2]\n",
    "            filename = ref_path_obj.name\n",
    "            reference_mapping[(category, filename)] = ref_path\n",
    "    \n",
    "    target_files = {}\n",
    "    if target_embeddings_dir.exists():\n",
    "        for npy_file in target_embeddings_dir.rglob(\"*.npy\"):\n",
    "            rel_path = npy_file.relative_to(target_embeddings_dir)\n",
    "            if len(rel_path.parts) >= 2:\n",
    "                category = rel_path.parts[0]\n",
    "                filename = rel_path.name\n",
    "                if category not in target_files:\n",
    "                    target_files[category] = {}\n",
    "                target_files[category][filename] = str(rel_path)\n",
    "    \n",
    "    matched_paths = []\n",
    "    for (category, filename), ref_path in reference_mapping.items():\n",
    "        if category in target_files and filename in target_files[category]:\n",
    "            matched_paths.append(target_files[category][filename])\n",
    "    \n",
    "    print(f\"Matched {len(matched_paths)} files ({len(matched_paths)/len(reference_mapping)*100:.1f}%)\")\n",
    "    matched_paths.sort()\n",
    "    \n",
    "    return matched_paths\n",
    "\n",
    "def load_single_embedding(args):\n",
    "    \"\"\"Load a single embedding file (worker function for parallel processing)\"\"\"\n",
    "    path, embeddings_dir, is_absolute = args\n",
    "    \n",
    "    try:\n",
    "        if is_absolute:\n",
    "            full_path = Path(path)\n",
    "        else:\n",
    "            full_path = Path(embeddings_dir) / path\n",
    "        \n",
    "        if not full_path.exists():\n",
    "            return None, None\n",
    "        \n",
    "        path_parts = full_path.parts\n",
    "        if len(path_parts) < 2:\n",
    "            return None, None\n",
    "        category = path_parts[-2]\n",
    "        \n",
    "        embedding = np.load(full_path)\n",
    "        if embedding.ndim > 1:\n",
    "            embedding = embedding.flatten()\n",
    "        \n",
    "        return category, embedding\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def load_embeddings_by_category(embedding_paths, embeddings_dir, num_workers=None, use_parallel=True):\n",
    "    \"\"\"Load embeddings grouped by category\"\"\"\n",
    "    print(\"Loading embeddings by category...\")\n",
    "    embeddings_by_category = defaultdict(list)\n",
    "    \n",
    "    embeddings_dir = Path(embeddings_dir)\n",
    "    \n",
    "    if num_workers is None:\n",
    "        num_workers = min(16, mp.cpu_count())\n",
    "    \n",
    "    path_args = []\n",
    "    for path in embedding_paths:\n",
    "        is_absolute = Path(path).is_absolute()\n",
    "        path_args.append((path, str(embeddings_dir), is_absolute))\n",
    "    \n",
    "    if use_parallel and len(embedding_paths) > 100:\n",
    "        print(f\"Using {num_workers} parallel workers...\")\n",
    "        print(f\"Processing {len(path_args):,} embedding files...\")\n",
    "        \n",
    "        chunk_size = max(5000, num_workers * 200)\n",
    "        total_chunks = (len(path_args) + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            for chunk_idx in range(0, len(path_args), chunk_size):\n",
    "                chunk = path_args[chunk_idx:chunk_idx + chunk_size]\n",
    "                chunk_num = (chunk_idx // chunk_size) + 1\n",
    "                \n",
    "                results = list(executor.map(load_single_embedding, chunk))\n",
    "                \n",
    "                for category, embedding in results:\n",
    "                    if category is not None and embedding is not None:\n",
    "                        embeddings_by_category[category].append(embedding)\n",
    "                        successful += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                \n",
    "                if chunk_num % 50 == 0 or chunk_num == total_chunks:\n",
    "                    progress = (chunk_num / total_chunks) * 100\n",
    "                    print(f\"Progress: {progress:.1f}% ({chunk_num}/{total_chunks} chunks, {successful:,} loaded)\")\n",
    "        \n",
    "        if failed > 0:\n",
    "            print(f\"\\nWarning: Failed to load {failed:,} embeddings\")\n",
    "        print(f\"Successfully loaded {successful:,} embeddings\")\n",
    "    else:\n",
    "        print(\"Using sequential loading...\")\n",
    "        for args in tqdm(path_args, desc=\"Loading embeddings\"):\n",
    "            category, embedding = load_single_embedding(args)\n",
    "            if category is not None and embedding is not None:\n",
    "                embeddings_by_category[category].append(embedding)\n",
    "    \n",
    "    print(f\"\\nLoaded embeddings for {len(embeddings_by_category)} categories:\")\n",
    "    for category, emb_list in sorted(embeddings_by_category.items()):\n",
    "        print(f\"  {category}: {len(emb_list)} embeddings\")\n",
    "    \n",
    "    return embeddings_by_category\n",
    "\n",
    "def compute_category_averages(embeddings_by_category):\n",
    "    \"\"\"Compute average embedding for each category\"\"\"\n",
    "    print(\"\\nComputing category average embeddings...\")\n",
    "    category_averages = {}\n",
    "    categories = []\n",
    "    \n",
    "    for category, emb_list in sorted(embeddings_by_category.items()):\n",
    "        if len(emb_list) == 0:\n",
    "            continue\n",
    "        \n",
    "        emb_array = np.array(emb_list)\n",
    "        avg_embedding = np.mean(emb_array, axis=0)\n",
    "        category_averages[category] = avg_embedding\n",
    "        categories.append(category)\n",
    "        \n",
    "        print(f\"  {category}: {len(emb_list)} embeddings -> shape {avg_embedding.shape}\")\n",
    "    \n",
    "    return category_averages, categories\n",
    "\n",
    "def compute_similarity_matrix(category_averages, categories):\n",
    "    \"\"\"Compute pairwise cosine similarity matrix\"\"\"\n",
    "    print(\"\\nComputing cosine similarity matrix...\")\n",
    "    \n",
    "    embeddings = np.array([category_averages[cat] for cat in categories])\n",
    "    \n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized_embeddings = embeddings / (norms + 1e-10)\n",
    "    \n",
    "    similarity_matrix = np.dot(normalized_embeddings, normalized_embeddings.T)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_distance_matrix(similarity_matrix):\n",
    "    \"\"\"Compute cosine distance matrix from similarity matrix\"\"\"\n",
    "    print(\"Computing cosine distance matrix...\")\n",
    "    \n",
    "    distance_matrix = 1 - similarity_matrix\n",
    "    np.fill_diagonal(distance_matrix, 0)\n",
    "    distance_matrix = (distance_matrix + distance_matrix.T) / 2\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "def load_category_types(cdi_path):\n",
    "    \"\"\"Load category type information from CDI words CSV\"\"\"\n",
    "    print(f\"\\nLoading category types from {cdi_path}...\")\n",
    "    cdi_df = pd.read_csv(cdi_path)\n",
    "    \n",
    "    category_types = {}\n",
    "    for _, row in cdi_df.iterrows():\n",
    "        category_types[row['uni_lemma']] = {\n",
    "            'is_animate': bool(row.get('is_animate', 0)),\n",
    "            'is_bodypart': bool(row.get('is_bodypart', 0)),\n",
    "            'is_small': bool(row.get('is_small', 0)),\n",
    "            'is_big': bool(row.get('is_big', 0))\n",
    "        }\n",
    "    \n",
    "    print(f\"Loaded type information for {len(category_types)} categories\")\n",
    "    return category_types\n",
    "\n",
    "def organize_categories_by_type(categories, category_types):\n",
    "    \"\"\"Organize categories into groups: animate, small, bodyparts, big\"\"\"\n",
    "    print(\"\\nOrganizing categories by type...\")\n",
    "    \n",
    "    organized = {\n",
    "        'animate': [],\n",
    "        'small': [],\n",
    "        'bodyparts': [],\n",
    "        'big': [],\n",
    "        'others': []\n",
    "    }\n",
    "    \n",
    "    for cat in categories:\n",
    "        if cat not in category_types:\n",
    "            organized['others'].append(cat)\n",
    "            continue\n",
    "        \n",
    "        types = category_types[cat]\n",
    "        if types['is_animate']:\n",
    "            organized['animate'].append(cat)\n",
    "        elif types['is_bodypart']:\n",
    "            organized['bodyparts'].append(cat)\n",
    "        elif types['is_small']:\n",
    "            organized['small'].append(cat)\n",
    "        elif types['is_big']:\n",
    "            organized['big'].append(cat)\n",
    "        else:\n",
    "            organized['others'].append(cat)\n",
    "    \n",
    "    for key in organized:\n",
    "        organized[key] = sorted(organized[key])\n",
    "        print(f\"  {key}: {len(organized[key])} categories\")\n",
    "    \n",
    "    return organized\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute RDM Matrices\n",
    "\n",
    "This step computes pairwise RDM matrices from category embeddings. You can run this for CLIP, DINOv3, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Compute CLIP RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CLIP RDM\n",
    "if CLIP_EMBEDDINGS_DIR:\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPUTING CLIP RDM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load embedding paths\n",
    "    if CLIP_EMBEDDING_LIST:\n",
    "        embedding_paths = load_embedding_paths(CLIP_EMBEDDING_LIST)\n",
    "    else:\n",
    "        embedding_paths = scan_embedding_directory(CLIP_EMBEDDINGS_DIR)\n",
    "    \n",
    "    # Load embeddings by category\n",
    "    embeddings_by_category = load_embeddings_by_category(\n",
    "        embedding_paths, \n",
    "        CLIP_EMBEDDINGS_DIR,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        use_parallel=USE_PARALLEL\n",
    "    )\n",
    "    \n",
    "    # Compute category averages\n",
    "    category_averages, categories = compute_category_averages(embeddings_by_category)\n",
    "    \n",
    "    # Save category averages\n",
    "    output_dir = Path(CLIP_OUTPUT_DIR)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    print(\"\\nSaving category average embeddings...\")\n",
    "    embeddings = np.array([category_averages[cat] for cat in categories])\n",
    "    npz_path = output_dir / 'category_average_embeddings.npz'\n",
    "    np.savez(npz_path, \n",
    "             embeddings=embeddings, \n",
    "             categories=np.array(categories))\n",
    "    print(f\"  Saved to {npz_path}\")\n",
    "    \n",
    "    # Compute similarity and distance matrices\n",
    "    similarity_matrix = compute_similarity_matrix(category_averages, categories)\n",
    "    distance_matrix = compute_distance_matrix(similarity_matrix)\n",
    "    \n",
    "    # Save matrices\n",
    "    print(\"\\nSaving data files...\")\n",
    "    np.save(output_dir / 'similarity_matrix.npy', similarity_matrix)\n",
    "    np.save(output_dir / 'distance_matrix.npy', distance_matrix)\n",
    "    \n",
    "    sim_df = pd.DataFrame(similarity_matrix, index=categories, columns=categories)\n",
    "    sim_df.to_csv(output_dir / 'similarity_matrix.csv')\n",
    "    \n",
    "    dist_df = pd.DataFrame(distance_matrix, index=categories, columns=categories)\n",
    "    dist_df.to_csv(output_dir / 'distance_matrix.csv')\n",
    "    \n",
    "    # Load category types and create organized RDM\n",
    "    cdi_path = Path(CDI_PATH)\n",
    "    if cdi_path.exists():\n",
    "        category_types = load_category_types(cdi_path)\n",
    "        organized_categories = organize_categories_by_type(categories, category_types)\n",
    "        \n",
    "        # Create ordered list\n",
    "        ordered_categories = (\n",
    "            organized_categories['animate'] +\n",
    "            organized_categories['bodyparts'] +\n",
    "            organized_categories['small'] +\n",
    "            organized_categories['big'] +\n",
    "            organized_categories['others']\n",
    "        )\n",
    "        \n",
    "        # Reorder matrices\n",
    "        cat_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "        ordered_indices = [cat_to_idx[cat] for cat in ordered_categories if cat in cat_to_idx]\n",
    "        reordered_matrix = distance_matrix[np.ix_(ordered_indices, ordered_indices)]\n",
    "        reordered_categories = [cat for cat in ordered_categories if cat in cat_to_idx]\n",
    "        \n",
    "        # Create heatmap\n",
    "        n_categories = len(reordered_categories)\n",
    "        fig_size = max(20, n_categories * 0.5)\n",
    "        \n",
    "        plt.figure(figsize=(fig_size, fig_size))\n",
    "        ax = sns.heatmap(reordered_matrix, \n",
    "                    xticklabels=reordered_categories,\n",
    "                    yticklabels=reordered_categories,\n",
    "                    cmap='RdYlBu_r',\n",
    "                    vmin=0,\n",
    "                    vmax=2,\n",
    "                    square=True,\n",
    "                    cbar_kws={'label': 'Cosine Distance', 'shrink': 0.8})\n",
    "        \n",
    "        plt.title('CLIP Category RDM (Organized by Type)', fontsize=24, pad=20)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.yticks(rotation=0, fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'rdm_organized_by_type.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved organized RDM to {output_dir / 'rdm_organized_by_type.png'}\")\n",
    "    \n",
    "    # Create full RDM\n",
    "    n_categories = len(categories)\n",
    "    fig_size = max(20, n_categories * 0.5)\n",
    "    \n",
    "    plt.figure(figsize=(fig_size, fig_size))\n",
    "    ax = sns.heatmap(distance_matrix, \n",
    "                xticklabels=categories,\n",
    "                yticklabels=categories,\n",
    "                cmap='RdYlBu_r',\n",
    "                vmin=0,\n",
    "                vmax=2,\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Cosine Distance', 'shrink': 0.8})\n",
    "    \n",
    "    plt.title('CLIP Category RDM (Full)', fontsize=24, pad=20)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(rotation=0, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'rdm_full.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved full RDM to {output_dir / 'rdm_full.png'}\")\n",
    "    \n",
    "    print(f\"\\nCLIP RDM computation complete! Results saved to {output_dir}\")\n",
    "    print(f\"Total categories: {len(categories)}\")\n",
    "    print(f\"Mean similarity: {similarity_matrix.mean():.4f}\")\n",
    "    print(f\"Mean distance: {distance_matrix.mean():.4f}\")\n",
    "else:\n",
    "    print(\"Skipping CLIP RDM computation (CLIP_EMBEDDINGS_DIR not set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Compute DINOv3 RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DINOv3 RDM\n",
    "if DINOV3_EMBEDDINGS_DIR:\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPUTING DINOv3 RDM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load embedding paths\n",
    "    if MATCH_FROM_CLIP_LIST and CLIP_EMBEDDING_LIST:\n",
    "        embedding_paths = match_embedding_paths_from_list(CLIP_EMBEDDING_LIST, DINOV3_EMBEDDINGS_DIR)\n",
    "    elif DINOV3_EMBEDDING_LIST:\n",
    "        embedding_paths = load_embedding_paths(DINOV3_EMBEDDING_LIST)\n",
    "    else:\n",
    "        embedding_paths = scan_embedding_directory(DINOV3_EMBEDDINGS_DIR)\n",
    "    \n",
    "    # Load embeddings by category\n",
    "    embeddings_by_category = load_embeddings_by_category(\n",
    "        embedding_paths, \n",
    "        DINOV3_EMBEDDINGS_DIR,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        use_parallel=USE_PARALLEL\n",
    "    )\n",
    "    \n",
    "    # Compute category averages\n",
    "    category_averages, categories = compute_category_averages(embeddings_by_category)\n",
    "    \n",
    "    # Save category averages\n",
    "    output_dir = Path(DINOV3_OUTPUT_DIR)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    print(\"\\nSaving category average embeddings...\")\n",
    "    embeddings = np.array([category_averages[cat] for cat in categories])\n",
    "    npz_path = output_dir / 'category_average_embeddings.npz'\n",
    "    np.savez(npz_path, \n",
    "             embeddings=embeddings, \n",
    "             categories=np.array(categories))\n",
    "    print(f\"  Saved to {npz_path}\")\n",
    "    \n",
    "    # Compute similarity and distance matrices\n",
    "    similarity_matrix = compute_similarity_matrix(category_averages, categories)\n",
    "    distance_matrix = compute_distance_matrix(similarity_matrix)\n",
    "    \n",
    "    # Save matrices\n",
    "    print(\"\\nSaving data files...\")\n",
    "    np.save(output_dir / 'similarity_matrix.npy', similarity_matrix)\n",
    "    np.save(output_dir / 'distance_matrix.npy', distance_matrix)\n",
    "    \n",
    "    sim_df = pd.DataFrame(similarity_matrix, index=categories, columns=categories)\n",
    "    sim_df.to_csv(output_dir / 'similarity_matrix.csv')\n",
    "    \n",
    "    dist_df = pd.DataFrame(distance_matrix, index=categories, columns=categories)\n",
    "    dist_df.to_csv(output_dir / 'distance_matrix.csv')\n",
    "    \n",
    "    # Load category types and create organized RDM\n",
    "    cdi_path = Path(CDI_PATH)\n",
    "    if cdi_path.exists():\n",
    "        category_types = load_category_types(cdi_path)\n",
    "        organized_categories = organize_categories_by_type(categories, category_types)\n",
    "        \n",
    "        # Create ordered list\n",
    "        ordered_categories = (\n",
    "            organized_categories['animate'] +\n",
    "            organized_categories['bodyparts'] +\n",
    "            organized_categories['small'] +\n",
    "            organized_categories['big'] +\n",
    "            organized_categories['others']\n",
    "        )\n",
    "        \n",
    "        # Reorder matrices\n",
    "        cat_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "        ordered_indices = [cat_to_idx[cat] for cat in ordered_categories if cat in cat_to_idx]\n",
    "        reordered_matrix = distance_matrix[np.ix_(ordered_indices, ordered_indices)]\n",
    "        reordered_categories = [cat for cat in ordered_categories if cat in cat_to_idx]\n",
    "        \n",
    "        # Create heatmap\n",
    "        n_categories = len(reordered_categories)\n",
    "        fig_size = max(20, n_categories * 0.5)\n",
    "        \n",
    "        plt.figure(figsize=(fig_size, fig_size))\n",
    "        ax = sns.heatmap(reordered_matrix, \n",
    "                    xticklabels=reordered_categories,\n",
    "                    yticklabels=reordered_categories,\n",
    "                    cmap='RdYlBu_r',\n",
    "                    vmin=0,\n",
    "                    vmax=2,\n",
    "                    square=True,\n",
    "                    cbar_kws={'label': 'Cosine Distance', 'shrink': 0.8})\n",
    "        \n",
    "        plt.title('DINOv3 Category RDM (Organized by Type)', fontsize=24, pad=20)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.yticks(rotation=0, fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'rdm_organized_by_type.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved organized RDM to {output_dir / 'rdm_organized_by_type.png'}\")\n",
    "    \n",
    "    # Create full RDM\n",
    "    n_categories = len(categories)\n",
    "    fig_size = max(20, n_categories * 0.5)\n",
    "    \n",
    "    plt.figure(figsize=(fig_size, fig_size))\n",
    "    ax = sns.heatmap(distance_matrix, \n",
    "                xticklabels=categories,\n",
    "                yticklabels=categories,\n",
    "                cmap='RdYlBu_r',\n",
    "                vmin=0,\n",
    "                vmax=2,\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Cosine Distance', 'shrink': 0.8})\n",
    "    \n",
    "    plt.title('DINOv3 Category RDM (Full)', fontsize=24, pad=20)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(rotation=0, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'rdm_full.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved full RDM to {output_dir / 'rdm_full.png'}\")\n",
    "    \n",
    "    print(f\"\\nDINOv3 RDM computation complete! Results saved to {output_dir}\")\n",
    "    print(f\"Total categories: {len(categories)}\")\n",
    "    print(f\"Mean similarity: {similarity_matrix.mean():.4f}\")\n",
    "    print(f\"Mean distance: {distance_matrix.mean():.4f}\")\n",
    "else:\n",
    "    print(\"Skipping DINOv3 RDM computation (DINOV3_EMBEDDINGS_DIR not set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Filter and Reorganize RDMs\n",
    "\n",
    "This step filters out low-quality categories and reorganizes the RDM by category type with optional hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and reorganize CLIP RDM\n",
    "if EXCLUSION_FILE or INCLUSION_FILE:\n",
    "    print(\"=\"*60)\n",
    "    print(\"FILTERING AND REORGANIZING CLIP RDM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load category averages\n",
    "    npz_path = Path(CLIP_OUTPUT_DIR) / 'category_average_embeddings.npz'\n",
    "    if not npz_path.exists():\n",
    "        print(f\"Error: {npz_path} not found. Please run Step 1 first.\")\n",
    "    else:\n",
    "        print(f\"Loading category averages from {npz_path}...\")\n",
    "        data = np.load(npz_path)\n",
    "        embeddings = data['embeddings']\n",
    "        categories = [str(cat) for cat in data['categories']]\n",
    "        \n",
    "        # Load exclusion/inclusion lists\n",
    "        if INCLUSION_FILE:\n",
    "            print(f\"Loading included categories from {INCLUSION_FILE}...\")\n",
    "            with open(INCLUSION_FILE, 'r') as f:\n",
    "                included_categories = set(line.strip() for line in f if line.strip())\n",
    "            print(f\"Found {len(included_categories)} categories to include\")\n",
    "            excluded_categories = set()\n",
    "        else:\n",
    "            print(f\"Loading excluded categories from {EXCLUSION_FILE}...\")\n",
    "            with open(EXCLUSION_FILE, 'r') as f:\n",
    "                excluded_categories = set(line.strip() for line in f if line.strip())\n",
    "            print(f\"Found {len(excluded_categories)} categories to exclude\")\n",
    "            included_categories = None\n",
    "        \n",
    "        # Filter categories\n",
    "        if included_categories is not None:\n",
    "            filtered_indices = [i for i, cat in enumerate(categories) if cat in included_categories]\n",
    "            filtered_categories = [categories[i] for i in filtered_indices]\n",
    "            filtered_embeddings = embeddings[filtered_indices]\n",
    "            print(f\"After filtering: {len(filtered_categories)} categories\")\n",
    "        else:\n",
    "            filtered_indices = [i for i, cat in enumerate(categories) if cat not in excluded_categories]\n",
    "            filtered_categories = [categories[i] for i in filtered_indices]\n",
    "            filtered_embeddings = embeddings[filtered_indices]\n",
    "            print(f\"After filtering: {len(filtered_categories)} categories (excluded {len(excluded_categories)})\")\n",
    "        \n",
    "        # Load category types\n",
    "        cdi_path = Path(CDI_PATH)\n",
    "        if cdi_path.exists():\n",
    "            category_types = load_category_types(cdi_path)\n",
    "            \n",
    "            # Organize by type\n",
    "            organized = {\n",
    "                'animals': [],\n",
    "                'bodyparts': [],\n",
    "                'big_objects': [],\n",
    "                'small_objects': [],\n",
    "                'others': []\n",
    "            }\n",
    "            \n",
    "            cat_to_embedding = {cat: emb for cat, emb in zip(filtered_categories, filtered_embeddings)}\n",
    "            \n",
    "            for cat in filtered_categories:\n",
    "                if cat not in category_types:\n",
    "                    organized['others'].append(cat)\n",
    "                    continue\n",
    "                \n",
    "                types = category_types[cat]\n",
    "                if types['is_animate']:\n",
    "                    organized['animals'].append(cat)\n",
    "                elif types['is_bodypart']:\n",
    "                    organized['bodyparts'].append(cat)\n",
    "                elif types['is_big']:\n",
    "                    organized['big_objects'].append(cat)\n",
    "                elif types['is_small']:\n",
    "                    organized['small_objects'].append(cat)\n",
    "                else:\n",
    "                    organized['others'].append(cat)\n",
    "            \n",
    "            # Optional hierarchical clustering within groups\n",
    "            if USE_CLUSTERING:\n",
    "                def cluster_categories_within_group(group_categories, cat_to_embedding):\n",
    "                    if len(group_categories) <= 1:\n",
    "                        return group_categories\n",
    "                    \n",
    "                    group_embeddings = np.array([cat_to_embedding[cat] for cat in group_categories])\n",
    "                    normalized_embeddings = (group_embeddings - group_embeddings.mean(axis=0)) / (group_embeddings.std(axis=0) + 1e-10)\n",
    "                    similarity_matrix = cosine_similarity(normalized_embeddings)\n",
    "                    distance_matrix = 1 - similarity_matrix\n",
    "                    np.fill_diagonal(distance_matrix, 0)\n",
    "                    \n",
    "                    condensed_distances = squareform(distance_matrix)\n",
    "                    linkage_matrix = linkage(condensed_distances, method='ward')\n",
    "                    \n",
    "                    try:\n",
    "                        linkage_matrix = optimal_leaf_ordering(linkage_matrix, condensed_distances)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    dendro_dict = dendrogram(linkage_matrix, no_plot=True)\n",
    "                    leaf_order = dendro_dict['leaves']\n",
    "                    \n",
    "                    return [group_categories[i] for i in leaf_order]\n",
    "                \n",
    "                for key in organized:\n",
    "                    if len(organized[key]) > 1:\n",
    "                        organized[key] = cluster_categories_within_group(organized[key], cat_to_embedding)\n",
    "            else:\n",
    "                for key in organized:\n",
    "                    organized[key] = sorted(organized[key])\n",
    "            \n",
    "            # Create ordered list\n",
    "            ordered_categories = (\n",
    "                organized['animals'] +\n",
    "                organized['bodyparts'] +\n",
    "                organized['big_objects'] +\n",
    "                organized['small_objects'] +\n",
    "                organized['others']\n",
    "            )\n",
    "            \n",
    "            ordered_embeddings = np.array([cat_to_embedding[cat] for cat in ordered_categories])\n",
    "        else:\n",
    "            ordered_categories = sorted(filtered_categories)\n",
    "            ordered_embeddings = filtered_embeddings\n",
    "            organized = {'animals': [], 'bodyparts': [], 'big_objects': [], 'small_objects': [], 'others': ordered_categories}\n",
    "        \n",
    "        # Compute similarity and distance matrices\n",
    "        normalized_embeddings = (ordered_embeddings - ordered_embeddings.mean(axis=0)) / (ordered_embeddings.std(axis=0) + 1e-10)\n",
    "        similarity_matrix = cosine_similarity(normalized_embeddings)\n",
    "        distance_matrix = 1 - similarity_matrix\n",
    "        np.fill_diagonal(distance_matrix, 0)\n",
    "        distance_matrix = (distance_matrix + distance_matrix.T) / 2\n",
    "        \n",
    "        # Save filtered data\n",
    "        output_dir = Path(FILTERED_OUTPUT_DIR)\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        print(\"\\nSaving filtered data files...\")\n",
    "        np.save(output_dir / 'similarity_matrix_filtered.npy', similarity_matrix)\n",
    "        np.save(output_dir / 'distance_matrix_filtered.npy', distance_matrix)\n",
    "        \n",
    "        sim_df = pd.DataFrame(similarity_matrix, index=ordered_categories, columns=ordered_categories)\n",
    "        sim_df.to_csv(output_dir / 'similarity_matrix_filtered.csv')\n",
    "        \n",
    "        dist_df = pd.DataFrame(distance_matrix, index=ordered_categories, columns=ordered_categories)\n",
    "        dist_df.to_csv(output_dir / 'distance_matrix_filtered.csv')\n",
    "        \n",
    "        # Create organized RDM heatmap\n",
    "        n_categories = len(ordered_categories)\n",
    "        fig_size = max(20, n_categories * 0.5)\n",
    "        \n",
    "        plt.figure(figsize=(fig_size, fig_size))\n",
    "        ax = sns.heatmap(distance_matrix, \n",
    "                    xticklabels=ordered_categories,\n",
    "                    yticklabels=ordered_categories,\n",
    "                    cmap='viridis',\n",
    "                    vmin=0,\n",
    "                    vmax=2,\n",
    "                    square=True,\n",
    "                    cbar_kws={'label': 'Distance (1 - Cosine Similarity)', 'shrink': 0.8})\n",
    "        \n",
    "        plt.title('CLIP Category RDM (Filtered and Organized)', fontsize=24, pad=20)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "        plt.yticks(rotation=0, fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'rdm_organized_filtered.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved filtered RDM to {output_dir / 'rdm_organized_filtered.png'}\")\n",
    "        \n",
    "        print(f\"\\nFiltering complete! Results saved to {output_dir}\")\n",
    "        print(f\"Original categories: {len(categories)}\")\n",
    "        print(f\"Filtered categories: {len(ordered_categories)}\")\n",
    "        print(f\"Mean distance: {distance_matrix.mean():.4f}\")\n",
    "else:\n",
    "    print(\"Skipping filtering step (EXCLUSION_FILE and INCLUSION_FILE not set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Correlate RDM Matrices\n",
    "\n",
    "This step correlates two RDM matrices (e.g., CLIP vs DINOv3) using Pearson and Spearman correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate RDM matrices\n",
    "if CORRELATE_RDMS:\n",
    "    print(\"=\"*60)\n",
    "    print(\"CORRELATING RDM MATRICES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Determine which RDM files to use (filtered if available, otherwise full)\n",
    "    if Path(FILTERED_OUTPUT_DIR).exists() and (Path(FILTERED_OUTPUT_DIR) / 'distance_matrix_filtered.npy').exists():\n",
    "        rdm1_path = Path(FILTERED_OUTPUT_DIR) / 'distance_matrix_filtered.npy'\n",
    "        rdm2_path = Path(DINOV3_OUTPUT_DIR) / 'distance_matrix.npy'  # Assuming DINOv3 not filtered\n",
    "        print(\"Using filtered CLIP RDM and full DINOv3 RDM\")\n",
    "    else:\n",
    "        rdm1_path = Path(CLIP_OUTPUT_DIR) / 'distance_matrix.npy'\n",
    "        rdm2_path = Path(DINOV3_OUTPUT_DIR) / 'distance_matrix.npy'\n",
    "        print(\"Using full CLIP and DINOv3 RDMs\")\n",
    "    \n",
    "    if not rdm1_path.exists():\n",
    "        print(f\"Error: {rdm1_path} not found. Please run Step 1 first.\")\n",
    "    elif not rdm2_path.exists():\n",
    "        print(f\"Error: {rdm2_path} not found. Please run Step 1.2 first.\")\n",
    "    else:\n",
    "        # Load matrices\n",
    "        print(f\"Loading RDM 1 from {rdm1_path}...\")\n",
    "        matrix1 = np.load(rdm1_path)\n",
    "        print(f\"  Shape: {matrix1.shape}\")\n",
    "        \n",
    "        print(f\"Loading RDM 2 from {rdm2_path}...\")\n",
    "        matrix2 = np.load(rdm2_path)\n",
    "        print(f\"  Shape: {matrix2.shape}\")\n",
    "        \n",
    "        if matrix1.shape != matrix2.shape:\n",
    "            print(f\"Warning: Matrices have different shapes: {matrix1.shape} vs {matrix2.shape}\")\n",
    "            print(\"  Attempting to match by category names...\")\n",
    "            \n",
    "            # Try to load category names and match\n",
    "            if Path(FILTERED_OUTPUT_DIR).exists():\n",
    "                cat1_df = pd.read_csv(Path(FILTERED_OUTPUT_DIR) / 'distance_matrix_filtered.csv', index_col=0)\n",
    "            else:\n",
    "                cat1_df = pd.read_csv(Path(CLIP_OUTPUT_DIR) / 'distance_matrix.csv', index_col=0)\n",
    "            cat2_df = pd.read_csv(Path(DINOV3_OUTPUT_DIR) / 'distance_matrix.csv', index_col=0)\n",
    "            \n",
    "            common_cats = sorted(set(cat1_df.index) & set(cat2_df.index))\n",
    "            print(f\"  Found {len(common_cats)} common categories\")\n",
    "            \n",
    "            if len(common_cats) > 0:\n",
    "                matrix1 = cat1_df.loc[common_cats, common_cats].values\n",
    "                matrix2 = cat2_df.loc[common_cats, common_cats].values\n",
    "                print(f\"  Matched matrices to shape: {matrix1.shape}\")\n",
    "            else:\n",
    "                raise ValueError(\"No common categories found between matrices\")\n",
    "        \n",
    "        # Extract lower triangle (excluding diagonal)\n",
    "        vec1 = matrix1[np.tril_indices_from(matrix1, k=-1)]\n",
    "        vec2 = matrix2[np.tril_indices_from(matrix2, k=-1)]\n",
    "        \n",
    "        print(f\"\\nExtracted lower triangle: {len(vec1)} elements\")\n",
    "        \n",
    "        # Remove NaN/Inf values\n",
    "        mask = np.isfinite(vec1) & np.isfinite(vec2)\n",
    "        vec1_clean = vec1[mask]\n",
    "        vec2_clean = vec2[mask]\n",
    "        \n",
    "        print(f\"Valid elements: {len(vec1_clean)} / {len(vec1)}\")\n",
    "        \n",
    "        # Compute correlations\n",
    "        pearson_r, pearson_p = pearsonr(vec1_clean, vec2_clean)\n",
    "        spearman_r, spearman_p = spearmanr(vec1_clean, vec2_clean)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CORRELATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Pearson r:  {pearson_r:.6f} (p = {pearson_p:.2e})\")\n",
    "        print(f\"Spearman r: {spearman_r:.6f} (p = {spearman_p:.2e})\")\n",
    "        print(f\"\\nMatrix 1 stats: Mean={vec1_clean.mean():.6f}, Std={vec1_clean.std():.6f}\")\n",
    "        print(f\"Matrix 2 stats: Mean={vec2_clean.mean():.6f}, Std={vec2_clean.std():.6f}\")\n",
    "else:\n",
    "    print(\"Skipping RDM correlation (CORRELATE_RDMS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Correlate Category Embeddings\n",
    "\n",
    "This step correlates category-level average embeddings between two embedding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate category embeddings\n",
    "if CORRELATE_EMBEDDINGS:\n",
    "    print(\"=\"*60)\n",
    "    print(\"CORRELATING CATEGORY EMBEDDINGS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    embeddings1_path = Path(CLIP_OUTPUT_DIR) / 'category_average_embeddings.npz'\n",
    "    embeddings2_path = Path(DINOV3_OUTPUT_DIR) / 'category_average_embeddings.npz'\n",
    "    \n",
    "    if not embeddings1_path.exists():\n",
    "        print(f\"Error: {embeddings1_path} not found. Please run Step 1 first.\")\n",
    "    elif not embeddings2_path.exists():\n",
    "        print(f\"Error: {embeddings2_path} not found. Please run Step 1.2 first.\")\n",
    "    else:\n",
    "        # Load embeddings\n",
    "        print(f\"Loading embeddings 1 from {embeddings1_path}...\")\n",
    "        data1 = np.load(embeddings1_path)\n",
    "        embeddings1 = data1['embeddings']\n",
    "        categories1 = [str(cat) for cat in data1['categories']]\n",
    "        print(f\"  Categories: {len(categories1)}, Embedding dim: {embeddings1.shape[1]}\")\n",
    "        \n",
    "        print(f\"Loading embeddings 2 from {embeddings2_path}...\")\n",
    "        data2 = np.load(embeddings2_path)\n",
    "        embeddings2 = data2['embeddings']\n",
    "        categories2 = [str(cat) for cat in data2['categories']]\n",
    "        print(f\"  Categories: {len(categories2)}, Embedding dim: {embeddings2.shape[1]}\")\n",
    "        \n",
    "        # Check embedding dimensions\n",
    "        if embeddings1.shape[1] != embeddings2.shape[1]:\n",
    "            print(f\"Warning: Embedding dimensions differ: {embeddings1.shape[1]} vs {embeddings2.shape[1]}\")\n",
    "            min_dim = min(embeddings1.shape[1], embeddings2.shape[1])\n",
    "            embeddings1 = embeddings1[:, :min_dim]\n",
    "            embeddings2 = embeddings2[:, :min_dim]\n",
    "            print(f\"  Using first {min_dim} dimensions\")\n",
    "        \n",
    "        # Find matching categories\n",
    "        categories1_set = set(categories1)\n",
    "        categories2_set = set(categories2)\n",
    "        matching_categories = sorted(categories1_set & categories2_set)\n",
    "        \n",
    "        print(f\"\\nMatching categories: {len(matching_categories)}\")\n",
    "        \n",
    "        if len(matching_categories) == 0:\n",
    "            print(\"Error: No matching categories found!\")\n",
    "        else:\n",
    "            # Create mapping from category to index\n",
    "            cat_to_idx1 = {cat: idx for idx, cat in enumerate(categories1)}\n",
    "            cat_to_idx2 = {cat: idx for idx, cat in enumerate(categories2)}\n",
    "            \n",
    "            # Compute correlations for each matching category\n",
    "            per_category_results = []\n",
    "            all_pearson_rs = []\n",
    "            all_spearman_rs = []\n",
    "            all_cosine_sims = []\n",
    "            \n",
    "            for cat in matching_categories:\n",
    "                idx1 = cat_to_idx1[cat]\n",
    "                idx2 = cat_to_idx2[cat]\n",
    "                \n",
    "                vec1 = embeddings1[idx1]\n",
    "                vec2 = embeddings2[idx2]\n",
    "                \n",
    "                # Remove NaN/Inf\n",
    "                mask = np.isfinite(vec1) & np.isfinite(vec2)\n",
    "                vec1_clean = vec1[mask]\n",
    "                vec2_clean = vec2[mask]\n",
    "                \n",
    "                if len(vec1_clean) >= 3:\n",
    "                    pearson_r, pearson_p = pearsonr(vec1_clean, vec2_clean)\n",
    "                    spearman_r, spearman_p = spearmanr(vec1_clean, vec2_clean)\n",
    "                else:\n",
    "                    pearson_r, pearson_p = np.nan, np.nan\n",
    "                    spearman_r, spearman_p = np.nan, np.nan\n",
    "                \n",
    "                # Cosine similarity\n",
    "                if len(vec1_clean) > 0:\n",
    "                    vec1_2d = vec1_clean.reshape(1, -1)\n",
    "                    vec2_2d = vec2_clean.reshape(1, -1)\n",
    "                    cosine_sim = cosine_similarity(vec1_2d, vec2_2d)[0, 0]\n",
    "                else:\n",
    "                    cosine_sim = np.nan\n",
    "                \n",
    "                per_category_results.append({\n",
    "                    'category': cat,\n",
    "                    'pearson_r': pearson_r,\n",
    "                    'spearman_r': spearman_r,\n",
    "                    'cosine_similarity': cosine_sim\n",
    "                })\n",
    "                \n",
    "                if not np.isnan(pearson_r):\n",
    "                    all_pearson_rs.append(pearson_r)\n",
    "                if not np.isnan(spearman_r):\n",
    "                    all_spearman_rs.append(spearman_r)\n",
    "                if not np.isnan(cosine_sim):\n",
    "                    all_cosine_sims.append(cosine_sim)\n",
    "            \n",
    "            # Summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY STATISTICS\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Categories analyzed: {len(matching_categories)}\")\n",
    "            print(f\"\\nPearson Correlation:\")\n",
    "            print(f\"  Mean:   {np.nanmean(all_pearson_rs):.6f}\")\n",
    "            print(f\"  Std:    {np.nanstd(all_pearson_rs):.6f}\")\n",
    "            print(f\"  Median: {np.nanmedian(all_pearson_rs):.6f}\")\n",
    "            print(f\"  Min:    {np.nanmin(all_pearson_rs):.6f}\")\n",
    "            print(f\"  Max:    {np.nanmax(all_pearson_rs):.6f}\")\n",
    "            print(f\"\\nSpearman Correlation:\")\n",
    "            print(f\"  Mean:   {np.nanmean(all_spearman_rs):.6f}\")\n",
    "            print(f\"  Std:    {np.nanstd(all_spearman_rs):.6f}\")\n",
    "            print(f\"  Median: {np.nanmedian(all_spearman_rs):.6f}\")\n",
    "            print(f\"\\nCosine Similarity:\")\n",
    "            print(f\"  Mean:   {np.nanmean(all_cosine_sims):.6f}\")\n",
    "            print(f\"  Std:    {np.nanstd(all_cosine_sims):.6f}\")\n",
    "            print(f\"  Median: {np.nanmedian(all_cosine_sims):.6f}\")\n",
    "            \n",
    "            # Top and bottom categories\n",
    "            sorted_results = sorted(per_category_results, \n",
    "                                  key=lambda x: x['pearson_r'] if not np.isnan(x['pearson_r']) else -np.inf, \n",
    "                                  reverse=True)\n",
    "            \n",
    "            print(f\"\\n\\nTop 10 categories by Pearson correlation:\")\n",
    "            for i, result in enumerate(sorted_results[:10], 1):\n",
    "                print(f\"  {i:2d}. {result['category']:<30} r={result['pearson_r']:.6f}, cos={result['cosine_similarity']:.6f}\")\n",
    "            \n",
    "            print(f\"\\nBottom 10 categories by Pearson correlation:\")\n",
    "            for i, result in enumerate(sorted_results[-10:], len(sorted_results)-9):\n",
    "                print(f\"  {i:2d}. {result['category']:<30} r={result['pearson_r']:.6f}, cos={result['cosine_similarity']:.6f}\")\n",
    "else:\n",
    "    print(\"Skipping category embedding correlation (CORRELATE_EMBEDDINGS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete pipeline for RDM analysis. The main steps are:\n",
    "\n",
    "1. **Compute RDM Matrices** - Computes pairwise distance matrices from embeddings\n",
    "2. **Filter and Reorganize** - Filters categories and reorganizes by type\n",
    "3. **Correlate RDMs** - Compares RDM matrices between models\n",
    "4. **Correlate Embeddings** - Compares category-level embeddings\n",
    "\n",
    "All results are saved to the specified output directories. Check the output directories for:\n",
    "- Category average embeddings (`.npz` and `.csv`)\n",
    "- Similarity and distance matrices (`.npy` and `.csv`)\n",
    "- RDM heatmap visualizations (`.png`)\n",
    "- Correlation results (printed to console)\n",
    "\n",
    "For more details, see the README.md file in this directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
