{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Filter, Normalize, and Reorganize RDM\n",
        "\n",
        "This notebook filters out low-quality categories and reorganizes the RDM by category type with optional hierarchical clustering.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This step:\n",
        "1. Loads category average embeddings from Notebook 01\n",
        "2. Filters categories based on inclusion/exclusion lists\n",
        "3. Normalizes embeddings (z-score) and computes RDM matrices\n",
        "4. Saves alphabetically sorted RDM matrices (original ordering)\n",
        "5. Organizes categories by type (animals, bodyparts, big objects, small objects)\n",
        "   - **OR** loads a previously saved category order (if `USE_SAVED_ORDER=True`)\n",
        "6. Optionally applies hierarchical clustering within each group (skipped if using saved order)\n",
        "7. Saves reorganized RDM matrices (by category type with optional clustering)\n",
        "8. Saves the exact category order for reuse with other embeddings\n",
        "9. Creates visualizations\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Dual Ordering**: Saves both alphabetically sorted and reorganized (by type/clustering) versions\n",
        "- **Order Reuse**: Can save and load category orders to ensure consistent ordering across different embedding types (e.g., CLIP vs DINO)\n",
        "- **Flexible Organization**: Can organize by category type with optional hierarchical clustering, or use a pre-computed order\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This step requires:\n",
        "- Output from Notebook 1 (CLIP or DINOV3 average category embeddings)\n",
        "- CDI words CSV file for category type information (if not using saved order)\n",
        "- Inclusion or exclusion list file\n",
        "- (Optional) Previously saved category order file (if `USE_SAVED_ORDER=True`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, optimal_leaf_ordering\n",
        "from scipy.spatial.distance import squareform\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set matplotlib backend\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "print(\"All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions loaded!\n"
          ]
        }
      ],
      "source": [
        "def load_category_types(cdi_path):\n",
        "    \"\"\"Load category type information from CDI words CSV\"\"\"\n",
        "    print(f\"Loading category types from {cdi_path}...\")\n",
        "    cdi_df = pd.read_csv(cdi_path)\n",
        "    \n",
        "    category_types = {}\n",
        "    for _, row in cdi_df.iterrows():\n",
        "        category_types[row['uni_lemma']] = {\n",
        "            'is_animate': bool(row.get('is_animate', 0)),\n",
        "            'is_bodypart': bool(row.get('is_bodypart', 0)),\n",
        "            'is_small': bool(row.get('is_small', 0)),\n",
        "            'is_big': bool(row.get('is_big', 0))\n",
        "        }\n",
        "    \n",
        "    print(f\"Loaded type information for {len(category_types)} categories\")\n",
        "    return category_types\n",
        "\n",
        "def cluster_categories_within_group(group_categories, cat_to_embedding, save_dendrogram=False, output_dir=None, group_name=None):\n",
        "    \"\"\"\n",
        "    Perform hierarchical clustering within a group of categories.\n",
        "    \n",
        "    Args:\n",
        "        group_categories: List of category names in the group\n",
        "        cat_to_embedding: Dictionary mapping category names to embeddings\n",
        "        save_dendrogram: Whether to save dendrogram plot (default: False)\n",
        "        output_dir: Output directory for saving dendrogram (required if save_dendrogram=True)\n",
        "        group_name: Name of the group for saving dendrogram (required if save_dendrogram=True)\n",
        "    \n",
        "    Returns:\n",
        "        List of category names reordered according to clustering dendrogram\n",
        "    \"\"\"\n",
        "    if len(group_categories) <= 1:\n",
        "        return group_categories, None\n",
        "    \n",
        "    # Get embeddings for this group\n",
        "    group_embeddings = np.array([cat_to_embedding[cat] for cat in group_categories])\n",
        "    \n",
        "    # Normalize embeddings\n",
        "    normalized_embeddings = (group_embeddings - group_embeddings.mean(axis=0)) / (group_embeddings.std(axis=0) + 1e-10)\n",
        "    \n",
        "    # Compute distance matrix (1 - cosine similarity)\n",
        "    similarity_matrix = cosine_similarity(normalized_embeddings)\n",
        "    distance_matrix = 1 - similarity_matrix\n",
        "    np.fill_diagonal(distance_matrix, 0)\n",
        "    \n",
        "    # Convert to condensed form for linkage\n",
        "    condensed_distances = squareform(distance_matrix)\n",
        "    \n",
        "    # Perform hierarchical clustering\n",
        "    linkage_matrix = linkage(condensed_distances, method='ward')\n",
        "    \n",
        "    # Get optimal leaf ordering for better visualization\n",
        "    try:\n",
        "        linkage_matrix = optimal_leaf_ordering(linkage_matrix, condensed_distances)\n",
        "    except:\n",
        "        # If optimal leaf ordering fails, use original linkage\n",
        "        pass\n",
        "    \n",
        "    # Extract the order from the dendrogram\n",
        "    dendro_dict = dendrogram(linkage_matrix, no_plot=True)\n",
        "    leaf_order = dendro_dict['leaves']\n",
        "    \n",
        "    # Reorder categories according to clustering\n",
        "    clustered_categories = [group_categories[i] for i in leaf_order]\n",
        "    \n",
        "    # Save dendrogram if requested\n",
        "    if save_dendrogram and output_dir is not None and group_name is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        output_dir.mkdir(exist_ok=True, parents=True)\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        dendrogram(linkage_matrix, \n",
        "                  labels=group_categories,\n",
        "                  leaf_rotation=90,\n",
        "                  leaf_font_size=10)\n",
        "        plt.title(f'Hierarchical Clustering Dendrogram: {group_name.upper()}\\n({len(group_categories)} categories)',\n",
        "                 fontsize=16, pad=20)\n",
        "        plt.xlabel('Category', fontsize=12)\n",
        "        plt.ylabel('Distance', fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save as PNG\n",
        "        output_path_png = output_dir / f'dendrogram_{group_name}.png'\n",
        "        plt.savefig(output_path_png, dpi=300, bbox_inches='tight', pad_inches=0.2)\n",
        "        print(f\"    Saved dendrogram to {output_path_png}\")\n",
        "        \n",
        "        # Save as PDF\n",
        "        output_path_pdf = output_dir / f'dendrogram_{group_name}.pdf'\n",
        "        plt.savefig(output_path_pdf, bbox_inches='tight', pad_inches=0.2)\n",
        "        print(f\"    Saved dendrogram to {output_path_pdf}\")\n",
        "        \n",
        "        plt.close()\n",
        "    \n",
        "    return clustered_categories, linkage_matrix\n",
        "\n",
        "print(\"Helper functions loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "**Please update the paths below according to your setup:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded. Please review and update paths as needed.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Input: Path to .npz file containing category average embeddings from Step 1\n",
        "INPUT_EMBEDDING_PATH = \"../../data/things_dinov3_embeddings.npz\"  # Direct path to .npz file (e.g., \"things_clip_embeddings.npz\")\n",
        "\n",
        "# Output directory prefix (e.g., \"bv_clip\", \"bv_dinov3\", \"things_clip\", \"things_dinov3\")\n",
        "# The output directory will be constructed as: {OUTPUT_DIR_PREFIX}_filtered_zscored_hierarchical_{n_cats}cats/\n",
        "OUTPUT_DIR_PREFIX = \"./bv_things_comp_12252025/things_dinov3\"  # Change this for different datasets/models\n",
        "\n",
        "# CDI words CSV file (required for category type organization)\n",
        "CDI_PATH = \"../../data/cdi_words.csv\"\n",
        "\n",
        "# Filtering options\n",
        "EXCLUSION_FILE = None  # Path to text file with categories to exclude (one per line), or None\n",
        "INCLUSION_FILE = \"../../data/things_bv_overlap_categories_exclude_zero_precisions.txt\"  # Path to text file with categories to include (one per line), or None\n",
        "\n",
        "# Processing options\n",
        "USE_CLUSTERING = True  # Enable hierarchical clustering within category groups\n",
        "SAVE_DENDROGRAMS = True  # Save dendrogram plots for each category group\n",
        "\n",
        "# Category ordering options\n",
        "# Set USE_SAVED_ORDER=True to reuse a previously computed category order (e.g., from another embedding type)\n",
        "# This is useful for comparing different embeddings (e.g., CLIP vs DINO) with the same category ordering\n",
        "USE_SAVED_ORDER = True  # If True, load category order from SAVED_ORDER_PATH and skip organization/clustering\n",
        "SAVED_ORDER_PATH = \"./bv_things_comp_12252025/bv_clip_filtered_zscored_hierarchical_163cats/category_order_reorganized.txt\"  # Path to text file with category order (one category per line), or None\n",
        "# Example: SAVED_ORDER_PATH = \"./bv_things_comp_12252025/bv_clip_filtered_zscored_hierarchical_163cats/category_order_reorganized.txt\"\n",
        "\n",
        "print(\"Configuration loaded. Please review and update paths as needed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load average category embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 1/8] Loading category averages from ../../data/things_dinov3_embeddings.npz...\n",
            "  Found categories under key 'labels'\n",
            "  Loaded 205 categories with embeddings of shape (205, 768)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load category average embeddings from Step 1\n",
        "npz_path = Path(INPUT_EMBEDDING_PATH)\n",
        "if not npz_path.exists():\n",
        "    raise FileNotFoundError(f\"{npz_path} not found. Please run Step 1 first or check the path.\")\n",
        "\n",
        "print(f\"\\n[Step 1/8] Loading category averages from {npz_path}...\")\n",
        "data = np.load(npz_path)\n",
        "embeddings = data['embeddings']\n",
        "\n",
        "# Handle different key names (categories, category, labels, label)\n",
        "available_keys = list(data.keys())\n",
        "categories = None\n",
        "for key_name in ['categories', 'category', 'labels', 'label']:\n",
        "    if key_name in available_keys:\n",
        "        categories = data[key_name]\n",
        "        # Convert numpy array of strings to list\n",
        "        if categories.dtype == 'object':\n",
        "            categories = [str(cat) for cat in categories]\n",
        "        else:\n",
        "            categories = categories.tolist()\n",
        "        print(f\"  Found categories under key '{key_name}'\")\n",
        "        break\n",
        "\n",
        "if categories is None:\n",
        "    raise KeyError(f\"Expected 'categories', 'category', 'labels', or 'label' key in NPZ file. Available keys: {available_keys}\")\n",
        "\n",
        "print(f\"  Loaded {len(categories)} categories with embeddings of shape {embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Filter categories based on inclusion/exclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 2/8] Filtering categories...\n",
            "  Loading included categories from ../../data/things_bv_overlap_categories_exclude_zero_precisions.txt...\n",
            "  Found 163 categories to include\n",
            "  After filtering: 163 categories\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Filter categories based on inclusion/exclusion lists\n",
        "print(f\"\\n[Step 2/8] Filtering categories...\")\n",
        "if INCLUSION_FILE:\n",
        "    print(f\"  Loading included categories from {INCLUSION_FILE}...\")\n",
        "    with open(INCLUSION_FILE, 'r') as f:\n",
        "        included_categories = set(line.strip() for line in f if line.strip())\n",
        "    print(f\"  Found {len(included_categories)} categories to include\")\n",
        "    excluded_categories = set()\n",
        "elif EXCLUSION_FILE:\n",
        "    print(f\"  Loading excluded categories from {EXCLUSION_FILE}...\")\n",
        "    with open(EXCLUSION_FILE, 'r') as f:\n",
        "        excluded_categories = set(line.strip() for line in f if line.strip())\n",
        "    print(f\"  Found {len(excluded_categories)} categories to exclude\")\n",
        "    included_categories = None\n",
        "else:\n",
        "    print(\"  No filtering file specified. Using all categories.\")\n",
        "    included_categories = None\n",
        "    excluded_categories = set()\n",
        "\n",
        "# Filter categories\n",
        "if included_categories is not None:\n",
        "    filtered_indices = [i for i, cat in enumerate(categories) if cat in included_categories]\n",
        "    filtered_categories = [categories[i] for i in filtered_indices]\n",
        "    filtered_embeddings = embeddings[filtered_indices]\n",
        "    print(f\"  After filtering: {len(filtered_categories)} categories\")\n",
        "else:\n",
        "    filtered_indices = [i for i, cat in enumerate(categories) if cat not in excluded_categories]\n",
        "    filtered_categories = [categories[i] for i in filtered_indices]\n",
        "    filtered_embeddings = embeddings[filtered_indices]\n",
        "    print(f\"  After filtering: {len(filtered_categories)} categories (excluded {len(excluded_categories)})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Normalize embeddings (z-score normalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 3/8] Normalizing filtered embeddings...\n",
            "  Normalized embeddings shape: (163, 768)\n",
            "  Sorting categories alphabetically...\n",
            "  Categories and embeddings sorted alphabetically\n",
            "  Saved alphabetically sorted normalized embeddings to bv_things_comp_12252025/things_dinov3_filtered_zscored_hierarchical_163cats/normalized_filtered_embeddings_alphabetical.csv\n",
            "    CSV contains 163 categories with 768 embedding dimensions\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Normalize embeddings (z-score normalization)\n",
        "print(f\"\\n[Step 3/8] Normalizing filtered embeddings...\")\n",
        "normalized_filtered_embeddings = (filtered_embeddings - filtered_embeddings.mean(axis=0)) / (filtered_embeddings.std(axis=0) + 1e-10)\n",
        "print(f\"  Normalized embeddings shape: {normalized_filtered_embeddings.shape}\")\n",
        "\n",
        "# Sort categories alphabetically and reorder embeddings to match\n",
        "# This creates the \"original\" (alphabetically sorted) version before reorganization\n",
        "print(f\"  Sorting categories alphabetically...\")\n",
        "sorted_category_indices = sorted(range(len(filtered_categories)), key=lambda i: filtered_categories[i])\n",
        "categories_alphabetical = [filtered_categories[i] for i in sorted_category_indices]\n",
        "normalized_embeddings_alphabetical = normalized_filtered_embeddings[sorted_category_indices]\n",
        "\n",
        "# Update filtered_categories and normalized_filtered_embeddings to sorted versions\n",
        "# (These will be used for reorganization later)\n",
        "filtered_categories = categories_alphabetical\n",
        "normalized_filtered_embeddings = normalized_embeddings_alphabetical\n",
        "print(f\"  Categories and embeddings sorted alphabetically\")\n",
        "\n",
        "# Construct output directory name based on prefix\n",
        "n_cats = len(filtered_categories)\n",
        "output_dir_name = f\"{OUTPUT_DIR_PREFIX}_filtered_zscored_hierarchical_{n_cats}cats\"\n",
        "output_dir = Path(output_dir_name)\n",
        "output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save normalized embeddings as CSV with category names (alphabetically sorted)\n",
        "# Create DataFrame with category names as index and embedding dimensions as columns\n",
        "embedding_df = pd.DataFrame(\n",
        "    normalized_embeddings_alphabetical,\n",
        "    index=categories_alphabetical,\n",
        "    columns=[f'dim_{i}' for i in range(normalized_embeddings_alphabetical.shape[1])]\n",
        ")\n",
        "csv_path = output_dir / 'normalized_filtered_embeddings_alphabetical.csv'\n",
        "embedding_df.to_csv(csv_path)\n",
        "print(f\"  Saved alphabetically sorted normalized embeddings to {csv_path}\")\n",
        "print(f\"    CSV contains {len(categories_alphabetical)} categories with {normalized_embeddings_alphabetical.shape[1]} embedding dimensions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Compute RDM matrices on filtered, normalized embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 4/8] Computing RDM matrices on filtered, normalized embeddings...\n",
            "  Computed RDM matrices with shape (163, 163)\n",
            "  Mean distance: 0.9997\n",
            "  Std distance: 0.1025\n",
            "  Saving alphabetically sorted matrices (original ordering) to bv_things_comp_12252025/things_dinov3_filtered_zscored_hierarchical_163cats...\n",
            "  Saved alphabetically sorted similarity and distance matrices\n",
            "  Note: These matrices are in alphabetical order - will be reorganized by category type in Step 7\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Compute RDM matrices on filtered, normalized embeddings (alphabetically sorted)\n",
        "print(f\"\\n[Step 4/8] Computing RDM matrices on filtered, normalized embeddings...\")\n",
        "# Compute similarity and distance matrices using normalized embeddings\n",
        "# These matrices are in alphabetical order (original ordering before reorganization)\n",
        "similarity_matrix_alphabetical = cosine_similarity(normalized_embeddings_alphabetical)\n",
        "distance_matrix_alphabetical = 1 - similarity_matrix_alphabetical\n",
        "np.fill_diagonal(distance_matrix_alphabetical, 0)\n",
        "distance_matrix_alphabetical = (distance_matrix_alphabetical + distance_matrix_alphabetical.T) / 2\n",
        "\n",
        "print(f\"  Computed RDM matrices with shape {distance_matrix_alphabetical.shape}\")\n",
        "print(f\"  Mean distance: {distance_matrix_alphabetical.mean():.4f}\")\n",
        "print(f\"  Std distance: {distance_matrix_alphabetical.std():.4f}\")\n",
        "\n",
        "# Save alphabetically sorted matrices (original ordering before reorganization)\n",
        "print(f\"  Saving alphabetically sorted matrices (original ordering) to {output_dir}...\")\n",
        "np.save(output_dir / 'similarity_matrix_alphabetical.npy', similarity_matrix_alphabetical)\n",
        "np.save(output_dir / 'distance_matrix_alphabetical.npy', distance_matrix_alphabetical)\n",
        "\n",
        "sim_df_alphabetical = pd.DataFrame(similarity_matrix_alphabetical, index=categories_alphabetical, columns=categories_alphabetical)\n",
        "sim_df_alphabetical.to_csv(output_dir / 'similarity_matrix_alphabetical.csv')\n",
        "\n",
        "dist_df_alphabetical = pd.DataFrame(distance_matrix_alphabetical, index=categories_alphabetical, columns=categories_alphabetical)\n",
        "dist_df_alphabetical.to_csv(output_dir / 'distance_matrix_alphabetical.csv')\n",
        "print(f\"  Saved alphabetically sorted similarity and distance matrices\")\n",
        "print(f\"  Note: These matrices are in alphabetical order - will be reorganized by category type in Step 7\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Organize categories by type OR load saved order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 5/8] Organizing categories...\n",
            "  Loading saved category order from bv_things_comp_12252025/bv_clip_filtered_zscored_hierarchical_163cats/category_order_reorganized.txt...\n",
            "  Loaded 163 categories in saved order\n",
            "  Skipping organization and clustering steps (using saved order)\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Organize categories by type OR load saved order\n",
        "print(f\"\\n[Step 5/8] Organizing categories...\")\n",
        "\n",
        "# Check if we should load a saved order\n",
        "if USE_SAVED_ORDER and SAVED_ORDER_PATH is not None:\n",
        "    saved_order_path = Path(SAVED_ORDER_PATH)\n",
        "    if not saved_order_path.exists():\n",
        "        raise FileNotFoundError(f\"Saved order file not found: {saved_order_path}\")\n",
        "    \n",
        "    print(f\"  Loading saved category order from {saved_order_path}...\")\n",
        "    with open(saved_order_path, 'r') as f:\n",
        "        # Skip comment lines (lines starting with #)\n",
        "        saved_order = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]\n",
        "    \n",
        "    # Verify that all categories in saved order exist in our filtered categories\n",
        "    saved_order_set = set(saved_order)\n",
        "    categories_alphabetical_set = set(categories_alphabetical)\n",
        "    \n",
        "    if saved_order_set != categories_alphabetical_set:\n",
        "        missing_in_saved = categories_alphabetical_set - saved_order_set\n",
        "        extra_in_saved = saved_order_set - categories_alphabetical_set\n",
        "        if missing_in_saved:\n",
        "            print(f\"  Warning: {len(missing_in_saved)} categories in current data but not in saved order: {sorted(missing_in_saved)[:5]}...\")\n",
        "        if extra_in_saved:\n",
        "            print(f\"  Warning: {len(extra_in_saved)} categories in saved order but not in current data: {sorted(extra_in_saved)[:5]}...\")\n",
        "        raise ValueError(f\"Category mismatch! Saved order has {len(saved_order)} categories, current data has {len(categories_alphabetical)} categories.\")\n",
        "    \n",
        "    # Use the saved order\n",
        "    ordered_categories = saved_order\n",
        "    print(f\"  Loaded {len(ordered_categories)} categories in saved order\")\n",
        "    print(f\"  Skipping organization and clustering steps (using saved order)\")\n",
        "    \n",
        "    # Create a dummy organized dict for compatibility (won't be used)\n",
        "    organized = {'animals': [], 'bodyparts': [], 'big_objects': [], 'small_objects': [], 'others': []}\n",
        "    cat_to_embedding = {cat: emb for cat, emb in zip(categories_alphabetical, normalized_embeddings_alphabetical)}\n",
        "    \n",
        "else:\n",
        "    # Normal organization by type\n",
        "    print(f\"  Organizing categories by type...\")\n",
        "    cdi_path = Path(CDI_PATH)\n",
        "    # Output directory already created in Step 4\n",
        "\n",
        "    if cdi_path.exists():\n",
        "        category_types = load_category_types(cdi_path)\n",
        "        \n",
        "        # Organize by type\n",
        "        organized = {\n",
        "            'animals': [],\n",
        "            'bodyparts': [],\n",
        "            'big_objects': [],\n",
        "            'small_objects': [],\n",
        "            'others': []\n",
        "        }\n",
        "        \n",
        "        # Use normalized embeddings for the mapping (alphabetically sorted)\n",
        "        cat_to_embedding = {cat: emb for cat, emb in zip(categories_alphabetical, normalized_embeddings_alphabetical)}\n",
        "        \n",
        "        for cat in categories_alphabetical:\n",
        "            if cat not in category_types:\n",
        "                organized['others'].append(cat)\n",
        "                continue\n",
        "            \n",
        "            types = category_types[cat]\n",
        "            if types['is_animate']:\n",
        "                organized['animals'].append(cat)\n",
        "            elif types['is_bodypart']:\n",
        "                organized['bodyparts'].append(cat)\n",
        "            elif types['is_big']:\n",
        "                organized['big_objects'].append(cat)\n",
        "            elif types['is_small']:\n",
        "                organized['small_objects'].append(cat)\n",
        "            else:\n",
        "                organized['others'].append(cat)\n",
        "        \n",
        "        print(f\"  Organized into: {len(organized['animals'])} animals, {len(organized['bodyparts'])} bodyparts, \"\n",
        "              f\"{len(organized['big_objects'])} big objects, {len(organized['small_objects'])} small objects, \"\n",
        "              f\"{len(organized['others'])} others\")\n",
        "    else:\n",
        "        print(f\"  Warning: CDI path {cdi_path} not found. Skipping organization by type.\")\n",
        "        organized = {'animals': [], 'bodyparts': [], 'big_objects': [], 'small_objects': [], 'others': categories_alphabetical}\n",
        "        cat_to_embedding = {cat: emb for cat, emb in zip(categories_alphabetical, normalized_embeddings_alphabetical)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Optionally apply hierarchical clustering within each group (skip if using saved order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 6/8] Skipping clustering (using saved order)...\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Optionally apply hierarchical clustering within each group\n",
        "# Skip this step if using a saved order\n",
        "if USE_SAVED_ORDER and SAVED_ORDER_PATH is not None:\n",
        "    print(f\"\\n[Step 6/8] Skipping clustering (using saved order)...\")\n",
        "    # ordered_categories already set in Step 5\n",
        "else:\n",
        "    print(f\"\\n[Step 6/8] Applying hierarchical clustering within groups...\")\n",
        "    if USE_CLUSTERING:\n",
        "        for key in organized:\n",
        "            if len(organized[key]) > 1:\n",
        "                print(f\"  Clustering {key} ({len(organized[key])} categories)...\")\n",
        "                organized[key], _ = cluster_categories_within_group(\n",
        "                    organized[key], \n",
        "                    cat_to_embedding,\n",
        "                    save_dendrogram=SAVE_DENDROGRAMS,\n",
        "                    output_dir=output_dir,\n",
        "                    group_name=key\n",
        "                )\n",
        "    else:\n",
        "        for key in organized:\n",
        "            organized[key] = sorted(organized[key])\n",
        "    \n",
        "    # Create ordered list\n",
        "    ordered_categories = (\n",
        "        organized['animals'] +\n",
        "        organized['bodyparts'] +\n",
        "        organized['big_objects'] +\n",
        "        organized['small_objects'] +\n",
        "        organized['others']\n",
        "    )\n",
        "    \n",
        "    # Save category organization info\n",
        "    with open(output_dir / 'category_organization_filtered.txt', 'w') as f:\n",
        "        f.write(\"Category Organization:\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        for key in ['animals', 'bodyparts', 'big_objects', 'small_objects', 'others']:\n",
        "            f.write(f\"\\n{key.replace('_', ' ').title()} ({len(organized[key])}):\\n\")\n",
        "            for cat in organized[key]:\n",
        "                f.write(f\"  {cat}\\n\")\n",
        "    \n",
        "    with open(output_dir / 'category_names_filtered.txt', 'w') as f:\n",
        "        for cat in ordered_categories:\n",
        "            f.write(f\"{cat}\\n\")\n",
        "    \n",
        "    # Save the exact reorganized category order for reuse with other embeddings\n",
        "    # This allows applying the same ordering to different embedding types (e.g., CLIP vs DINO)\n",
        "    with open(output_dir / 'category_order_reorganized.txt', 'w') as f:\n",
        "        f.write(\"# Reorganized category order (after hierarchical clustering by category type)\\n\")\n",
        "        f.write(\"# This file can be loaded with USE_SAVED_ORDER=True to apply the same ordering to other embeddings\\n\")\n",
        "        f.write(\"# Format: one category name per line\\n\")\n",
        "        for cat in ordered_categories:\n",
        "            f.write(f\"{cat}\\n\")\n",
        "    print(f\"  Saved reorganized category order to {output_dir / 'category_order_reorganized.txt'}\")\n",
        "    print(f\"    This order can be reused with other embeddings by setting USE_SAVED_ORDER=True\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Compute and save filtered RDM matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 7/8] Reorganizing RDM matrices according to new category ordering...\n",
            "  Reorganized RDM matrices with shape (163, 163)\n",
            "  Saving reorganized matrices to bv_things_comp_12252025/things_dinov3_filtered_zscored_hierarchical_163cats...\n",
            "  Saved reorganized similarity and distance matrices\n",
            "  Note: Alphabetically sorted (original) matrices saved in Step 4 with '_alphabetical' suffix\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Reorganize RDM matrices according to new category ordering\n",
        "print(f\"\\n[Step 7/8] Reorganizing RDM matrices according to new category ordering...\")\n",
        "# Create mapping from alphabetical index to reorganized index\n",
        "alphabetical_to_reorganized_index = {cat: i for i, cat in enumerate(categories_alphabetical)}\n",
        "reorganized_indices = [alphabetical_to_reorganized_index[cat] for cat in ordered_categories]\n",
        "\n",
        "# Reorganize the matrices (computed in Step 4) according to the new ordering\n",
        "# These are the final reorganized matrices (by category type with optional clustering)\n",
        "similarity_matrix_reorganized = similarity_matrix_alphabetical[np.ix_(reorganized_indices, reorganized_indices)]\n",
        "distance_matrix_reorganized = distance_matrix_alphabetical[np.ix_(reorganized_indices, reorganized_indices)]\n",
        "\n",
        "print(f\"  Reorganized RDM matrices with shape {distance_matrix_reorganized.shape}\")\n",
        "print(f\"  Saving reorganized matrices to {output_dir}...\")\n",
        "np.save(output_dir / 'similarity_matrix_reorganized.npy', similarity_matrix_reorganized)\n",
        "np.save(output_dir / 'distance_matrix_reorganized.npy', distance_matrix_reorganized)\n",
        "\n",
        "sim_df_reorganized = pd.DataFrame(similarity_matrix_reorganized, index=ordered_categories, columns=ordered_categories)\n",
        "sim_df_reorganized.to_csv(output_dir / 'similarity_matrix_reorganized.csv')\n",
        "\n",
        "dist_df_reorganized = pd.DataFrame(distance_matrix_reorganized, index=ordered_categories, columns=ordered_categories)\n",
        "dist_df_reorganized.to_csv(output_dir / 'distance_matrix_reorganized.csv')\n",
        "print(f\"  Saved reorganized similarity and distance matrices\")\n",
        "print(f\"  Note: Alphabetically sorted (original) matrices saved in Step 4 with '_alphabetical' suffix\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Create visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 8/8] Creating visualizations...\n",
            "  Saved RDM heatmap to bv_things_comp_12252025/things_dinov3_filtered_zscored_hierarchical_163cats/rdm_reorganized.png\n",
            "  Saved RDM heatmap to bv_things_comp_12252025/things_dinov3_filtered_zscored_hierarchical_163cats/rdm_reorganized.pdf\n",
            "  Saved RDM heatmap (coolwarm) to bv_things_comp_12252025/things_dinov3_filtered_zscored_hierarchical_163cats/rdm_reorganized_coolwarm.png\n",
            "  Saved RDM heatmap (coolwarm) to bv_things_comp_12252025/things_dinov3_filtered_zscored_hierarchical_163cats/rdm_reorganized_coolwarm.pdf\n",
            "  Computing top similar/dissimilar pairs...\n",
            "\n",
            "============================================================\n",
            "Filtering complete! Results saved to bv_things_comp_12252025/things_dinov3_filtered_zscored_hierarchical_163cats\n",
            "============================================================\n",
            "Original categories: 205\n",
            "Filtered categories: 163\n",
            "Mean distance: 0.9997\n",
            "Std distance: 0.1025\n",
            "Min distance: 0.1995\n",
            "Max distance: 1.1740\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Create visualizations\n",
        "print(f\"\\n[Step 8/8] Creating visualizations...\")\n",
        "\n",
        "# Create organized RDM heatmap\n",
        "n_categories = len(ordered_categories)\n",
        "fig_size = max(20, n_categories * 0.5)\n",
        "\n",
        "# Set font size for axis labels (adaptive based on number of categories)\n",
        "# Larger font for fewer categories, smaller but still readable for many categories\n",
        "if n_categories <= 50:\n",
        "    label_fontsize = 14\n",
        "elif n_categories <= 100:\n",
        "    label_fontsize = 12\n",
        "elif n_categories <= 150:\n",
        "    label_fontsize = 10\n",
        "else:\n",
        "    label_fontsize = 9\n",
        "\n",
        "plt.figure(figsize=(fig_size, fig_size))\n",
        "ax = sns.heatmap(distance_matrix_reorganized, \n",
        "            xticklabels=ordered_categories,\n",
        "            yticklabels=ordered_categories,\n",
        "            cmap='viridis',\n",
        "            vmin=0,\n",
        "            vmax=2,\n",
        "            square=True,\n",
        "            cbar_kws={'label': 'Distance (1 - Cosine Similarity)', 'shrink': 0.8})\n",
        "\n",
        "plt.title(f'{OUTPUT_DIR_PREFIX.replace(\"_\", \" \").title()} Category RDM (Filtered and Organized)', fontsize=24, pad=20)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=label_fontsize)\n",
        "plt.yticks(rotation=0, fontsize=label_fontsize)\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'rdm_reorganized.png', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(output_dir / 'rdm_reorganized.pdf', bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  Saved RDM heatmap to {output_dir / 'rdm_reorganized.png'}\")\n",
        "print(f\"  Saved RDM heatmap to {output_dir / 'rdm_reorganized.pdf'}\")\n",
        "\n",
        "# Create coolwarm colormap version\n",
        "plt.figure(figsize=(fig_size, fig_size))\n",
        "ax = sns.heatmap(distance_matrix_reorganized, \n",
        "            xticklabels=ordered_categories,\n",
        "            yticklabels=ordered_categories,\n",
        "            cmap='coolwarm',\n",
        "            vmin=0,\n",
        "            vmax=2,\n",
        "            square=True,\n",
        "            cbar_kws={'label': 'Distance (1 - Cosine Similarity)', 'shrink': 0.8})\n",
        "\n",
        "plt.title(f'{OUTPUT_DIR_PREFIX.replace(\"_\", \" \").title()} Category RDM (Filtered and Organized)', fontsize=24, pad=20)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=label_fontsize)\n",
        "plt.yticks(rotation=0, fontsize=label_fontsize)\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'rdm_reorganized_coolwarm.png', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(output_dir / 'rdm_reorganized_coolwarm.pdf', bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  Saved RDM heatmap (coolwarm) to {output_dir / 'rdm_reorganized_coolwarm.png'}\")\n",
        "print(f\"  Saved RDM heatmap (coolwarm) to {output_dir / 'rdm_reorganized_coolwarm.pdf'}\")\n",
        "\n",
        "# Find and save top similar/dissimilar pairs\n",
        "print(f\"  Computing top similar/dissimilar pairs...\")\n",
        "# Get upper triangle (excluding diagonal)\n",
        "triu_indices = np.triu_indices_from(distance_matrix_reorganized, k=1)\n",
        "distances_flat = distance_matrix_reorganized[triu_indices]\n",
        "pairs_flat = [(ordered_categories[i], ordered_categories[j]) for i, j in zip(triu_indices[0], triu_indices[1])]\n",
        "\n",
        "# Sort by distance (ascending for similar, descending for dissimilar)\n",
        "sorted_indices_similar = np.argsort(distances_flat)\n",
        "sorted_indices_dissimilar = np.argsort(distances_flat)[::-1]\n",
        "\n",
        "for n in [20, 30, 50]:\n",
        "    # Top similar pairs\n",
        "    top_similar = [(pairs_flat[i], distances_flat[i]) for i in sorted_indices_similar[:n]]\n",
        "    with open(output_dir / f'top_{n}_similar_pairs.txt', 'w') as f:\n",
        "        f.write(f\"Top {n} Most Similar Category Pairs:\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        for (cat1, cat2), dist in top_similar:\n",
        "            f.write(f\"{cat1} <-> {cat2}: {dist:.4f}\\n\")\n",
        "    \n",
        "    # Top dissimilar pairs\n",
        "    top_dissimilar = [(pairs_flat[i], distances_flat[i]) for i in sorted_indices_dissimilar[:n]]\n",
        "    with open(output_dir / f'top_{n}_dissimilar_pairs.txt', 'w') as f:\n",
        "        f.write(f\"Top {n} Most Dissimilar Category Pairs:\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        for (cat1, cat2), dist in top_dissimilar:\n",
        "            f.write(f\"{cat1} <-> {cat2}: {dist:.4f}\\n\")\n",
        "    \n",
        "    # Visualize top similar pairs\n",
        "    if n <= 30:  # Only create visualizations for smaller numbers\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "        \n",
        "        # Similar pairs\n",
        "        similar_cats = list(set([cat for pair, _ in top_similar for cat in pair]))\n",
        "        similar_indices = [ordered_categories.index(cat) for cat in similar_cats if cat in ordered_categories]\n",
        "        similar_matrix = distance_matrix_reorganized[np.ix_(similar_indices, similar_indices)]\n",
        "        \n",
        "        sns.heatmap(similar_matrix, \n",
        "                   xticklabels=[ordered_categories[i] for i in similar_indices],\n",
        "                   yticklabels=[ordered_categories[i] for i in similar_indices],\n",
        "                   cmap='viridis', vmin=0, vmax=2, square=True, ax=axes[0],\n",
        "                   cbar_kws={'label': 'Distance'})\n",
        "        axes[0].set_title(f'Top {n} Similar Pairs', fontsize=14)\n",
        "        axes[0].tick_params(axis='x', rotation=45, labelsize=8)\n",
        "        axes[0].tick_params(axis='y', rotation=0, labelsize=8)\n",
        "        \n",
        "        # Dissimilar pairs\n",
        "        dissimilar_cats = list(set([cat for pair, _ in top_dissimilar for cat in pair]))\n",
        "        dissimilar_indices = [ordered_categories.index(cat) for cat in dissimilar_cats if cat in ordered_categories]\n",
        "        dissimilar_matrix = distance_matrix_reorganized[np.ix_(dissimilar_indices, dissimilar_indices)]\n",
        "        \n",
        "        sns.heatmap(dissimilar_matrix, \n",
        "                   xticklabels=[ordered_categories[i] for i in dissimilar_indices],\n",
        "                   yticklabels=[ordered_categories[i] for i in dissimilar_indices],\n",
        "                   cmap='viridis', vmin=0, vmax=2, square=True, ax=axes[1],\n",
        "                   cbar_kws={'label': 'Distance'})\n",
        "        axes[1].set_title(f'Top {n} Dissimilar Pairs', fontsize=14)\n",
        "        axes[1].tick_params(axis='x', rotation=45, labelsize=8)\n",
        "        axes[1].tick_params(axis='y', rotation=0, labelsize=8)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_dir / f'top_{n}_similar_pairs.png', dpi=300, bbox_inches='tight')\n",
        "        plt.savefig(output_dir / f'top_{n}_dissimilar_pairs.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Filtering complete! Results saved to {output_dir}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Original categories: {len(categories)}\")\n",
        "print(f\"Filtered categories: {len(ordered_categories)}\")\n",
        "print(f\"Mean distance: {distance_matrix_reorganized.mean():.4f}\")\n",
        "print(f\"Std distance: {distance_matrix_reorganized.std():.4f}\")\n",
        "print(f\"Min distance: {distance_matrix_reorganized[triu_indices].min():.4f}\")\n",
        "print(f\"Max distance: {distance_matrix_reorganized[triu_indices].max():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vislearnlabpy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
