{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Average Embeddings for CLIP and DINOv3\n",
    "\n",
    "This notebook computes category average embeddings from individual CLIP and DINOv3 embeddings.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This step:\n",
    "1. Loads individual CLIP embeddings from files\n",
    "2. Groups embeddings by category\n",
    "3. Computes category average embeddings for CLIP\n",
    "4. Loads individual DINOv3 embeddings from files\n",
    "5. Groups embeddings by category\n",
    "6. Computes category average embeddings for DINOv3\n",
    "7. Saves the average embeddings for later RDM computation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have the following packages installed:\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `tqdm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Please update the paths below according to your setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Paths for CLIP embeddings\n",
    "CLIP_EMBEDDING_LIST = \"/data2/dataset/babyview/868_hours/outputs/yoloe_cdi_embeddings/clip_embeddings_new/clip_image_embeddings_doc_normalized_filtered-by-clip-0.26.txt\"  # Path to text file with CLIP embedding paths (one per line), or None to scan directory\n",
    "CLIP_EMBEDDINGS_DIR = \"/data2/dataset/babyview/868_hours/outputs/yoloe_cdi_embeddings/clip_embeddings_new\"  # Base directory for CLIP embeddings\n",
    "CLIP_OUTPUT_DIR = \"./clip_rdm_results_26\"  # Directory where CLIP results will be saved\n",
    "\n",
    "# Paths for DINOv3 embeddings\n",
    "DINOV3_EMBEDDING_LIST = \"/data2/dataset/babyview/868_hours/outputs/yoloe_cdi_embeddings/clip_embeddings_new/clip_image_embeddings_doc_normalized_filtered-by-clip-0.26.txt\"  # Path to text file with DINOv3 embedding paths, or None to scan directory\n",
    "DINOV3_EMBEDDINGS_DIR = \"/data2/dataset/babyview/868_hours/outputs/yoloe_cdi_embeddings/facebook_dinov3-vitb16-pretrain-lvd1689m\"  # Base directory for DINOv3 embeddings\n",
    "DINOV3_OUTPUT_DIR = \"./dinov3_rdm_results_26\"  # Directory where DINOv3 results will be saved\n",
    "\n",
    "# Option to match from CLIP list (ensures same images for DINOv3)\n",
    "CLIP_EMBEDDING_LIST_REF = \"/data2/dataset/babyview/868_hours/outputs/yoloe_cdi_embeddings/clip_embeddings_new/clip_image_embeddings_doc_normalized_filtered-by-clip-0.26.txt\"  # Reference CLIP list for matching\n",
    "MATCH_FROM_CLIP_LIST = True  # If True, match DINOv3 filenames from CLIP list (ensures same images)\n",
    "\n",
    "# Processing options\n",
    "NUM_WORKERS = 24  # Number of parallel workers (None = auto-detect, max 16)\n",
    "USE_PARALLEL = True  # Enable parallel loading\n",
    "\n",
    "print(\"Configuration loaded. Please review and update paths as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_paths(txt_path):\n",
    "    \"\"\"Load embedding file paths from text file\"\"\"\n",
    "    print(f\"Loading embedding paths from {txt_path}...\")\n",
    "    with open(txt_path, 'r') as f:\n",
    "        paths = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Found {len(paths)} embedding paths\")\n",
    "    return paths\n",
    "\n",
    "def scan_embedding_directory(embeddings_dir):\n",
    "    \"\"\"Scan directory for all .npy embedding files\"\"\"\n",
    "    embeddings_dir = Path(embeddings_dir)\n",
    "    print(f\"Scanning {embeddings_dir} for .npy files...\")\n",
    "    \n",
    "    npy_files = list(embeddings_dir.rglob(\"*.npy\"))\n",
    "    \n",
    "    if len(npy_files) == 0:\n",
    "        raise ValueError(f\"No .npy files found in {embeddings_dir}\")\n",
    "    \n",
    "    paths = [str(f.relative_to(embeddings_dir)) for f in npy_files]\n",
    "    paths.sort()\n",
    "    \n",
    "    print(f\"Found {len(paths)} embedding files\")\n",
    "    return paths\n",
    "\n",
    "def match_embedding_paths_from_list(reference_list_path, target_embeddings_dir):\n",
    "    \"\"\"Match embedding paths from a reference list to target directory\"\"\"\n",
    "    target_embeddings_dir = Path(target_embeddings_dir)\n",
    "    \n",
    "    print(f\"Loading reference embedding list from {reference_list_path}...\")\n",
    "    with open(reference_list_path, 'r') as f:\n",
    "        reference_paths = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Found {len(reference_paths)} reference paths\")\n",
    "    \n",
    "    reference_mapping = {}\n",
    "    for ref_path in reference_paths:\n",
    "        ref_path_obj = Path(ref_path)\n",
    "        if len(ref_path_obj.parts) >= 2:\n",
    "            category = ref_path_obj.parts[-2]\n",
    "            filename = ref_path_obj.name\n",
    "            reference_mapping[(category, filename)] = ref_path\n",
    "    \n",
    "    target_files = {}\n",
    "    if target_embeddings_dir.exists():\n",
    "        for npy_file in target_embeddings_dir.rglob(\"*.npy\"):\n",
    "            rel_path = npy_file.relative_to(target_embeddings_dir)\n",
    "            if len(rel_path.parts) >= 2:\n",
    "                category = rel_path.parts[0]\n",
    "                filename = rel_path.name\n",
    "                if category not in target_files:\n",
    "                    target_files[category] = {}\n",
    "                target_files[category][filename] = str(rel_path)\n",
    "    \n",
    "    matched_paths = []\n",
    "    for (category, filename), ref_path in reference_mapping.items():\n",
    "        if category in target_files and filename in target_files[category]:\n",
    "            matched_paths.append(target_files[category][filename])\n",
    "    \n",
    "    print(f\"Matched {len(matched_paths)} files ({len(matched_paths)/len(reference_mapping)*100:.1f}%)\")\n",
    "    matched_paths.sort()\n",
    "    \n",
    "    return matched_paths\n",
    "\n",
    "def load_single_embedding(args):\n",
    "    \"\"\"Load a single embedding file (worker function for parallel processing)\"\"\"\n",
    "    path, embeddings_dir, is_absolute = args\n",
    "    \n",
    "    try:\n",
    "        if is_absolute:\n",
    "            full_path = Path(path)\n",
    "        else:\n",
    "            full_path = Path(embeddings_dir) / path\n",
    "        \n",
    "        if not full_path.exists():\n",
    "            return None, None\n",
    "        \n",
    "        path_parts = full_path.parts\n",
    "        if len(path_parts) < 2:\n",
    "            return None, None\n",
    "        category = path_parts[-2]\n",
    "        \n",
    "        embedding = np.load(full_path)\n",
    "        if embedding.ndim > 1:\n",
    "            embedding = embedding.flatten()\n",
    "        \n",
    "        return category, embedding\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def load_embeddings_by_category(embedding_paths, embeddings_dir, num_workers=None, use_parallel=True):\n",
    "    \"\"\"Load embeddings grouped by category\"\"\"\n",
    "    print(\"Loading embeddings by category...\")\n",
    "    embeddings_by_category = defaultdict(list)\n",
    "    \n",
    "    embeddings_dir = Path(embeddings_dir)\n",
    "    \n",
    "    if num_workers is None:\n",
    "        num_workers = min(16, mp.cpu_count())\n",
    "    \n",
    "    path_args = []\n",
    "    for path in embedding_paths:\n",
    "        is_absolute = Path(path).is_absolute()\n",
    "        path_args.append((path, str(embeddings_dir), is_absolute))\n",
    "    \n",
    "    if use_parallel and len(embedding_paths) > 100:\n",
    "        print(f\"Using {num_workers} parallel workers...\")\n",
    "        print(f\"Processing {len(path_args):,} embedding files...\")\n",
    "        \n",
    "        chunk_size = max(5000, num_workers * 200)\n",
    "        total_chunks = (len(path_args) + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            for chunk_idx in range(0, len(path_args), chunk_size):\n",
    "                chunk = path_args[chunk_idx:chunk_idx + chunk_size]\n",
    "                chunk_num = (chunk_idx // chunk_size) + 1\n",
    "                \n",
    "                results = list(executor.map(load_single_embedding, chunk))\n",
    "                \n",
    "                for category, embedding in results:\n",
    "                    if category is not None and embedding is not None:\n",
    "                        embeddings_by_category[category].append(embedding)\n",
    "                        successful += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                \n",
    "                if chunk_num % 50 == 0 or chunk_num == total_chunks:\n",
    "                    progress = (chunk_num / total_chunks) * 100\n",
    "                    print(f\"Progress: {progress:.1f}% ({chunk_num}/{total_chunks} chunks, {successful:,} loaded)\")\n",
    "        \n",
    "        if failed > 0:\n",
    "            print(f\"\\nWarning: Failed to load {failed:,} embeddings\")\n",
    "        print(f\"Successfully loaded {successful:,} embeddings\")\n",
    "    else:\n",
    "        print(\"Using sequential loading...\")\n",
    "        for args in tqdm(path_args, desc=\"Loading embeddings\"):\n",
    "            category, embedding = load_single_embedding(args)\n",
    "            if category is not None and embedding is not None:\n",
    "                embeddings_by_category[category].append(embedding)\n",
    "    \n",
    "    print(f\"\\nLoaded embeddings for {len(embeddings_by_category)} categories:\")\n",
    "    for category, emb_list in sorted(embeddings_by_category.items()):\n",
    "        print(f\"  {category}: {len(emb_list)} embeddings\")\n",
    "    \n",
    "    return embeddings_by_category\n",
    "\n",
    "def compute_category_averages(embeddings_by_category):\n",
    "    \"\"\"Compute average embedding for each category\"\"\"\n",
    "    print(\"\\nComputing category average embeddings...\")\n",
    "    category_averages = {}\n",
    "    categories = []\n",
    "    \n",
    "    for category, emb_list in sorted(embeddings_by_category.items()):\n",
    "        if len(emb_list) == 0:\n",
    "            continue\n",
    "        \n",
    "        emb_array = np.array(emb_list)\n",
    "        avg_embedding = np.mean(emb_array, axis=0)\n",
    "        category_averages[category] = avg_embedding\n",
    "        categories.append(category)\n",
    "        \n",
    "        print(f\"  {category}: {len(emb_list)} embeddings -> shape {avg_embedding.shape}\")\n",
    "    \n",
    "    return category_averages, categories\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute CLIP Average Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COMPUTING CLIP AVERAGE EMBEDDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load embedding paths\n",
    "if CLIP_EMBEDDING_LIST:\n",
    "    embedding_paths = load_embedding_paths(CLIP_EMBEDDING_LIST)\n",
    "else:\n",
    "    embedding_paths = scan_embedding_directory(CLIP_EMBEDDINGS_DIR)\n",
    "\n",
    "# Load embeddings by category\n",
    "embeddings_by_category = load_embeddings_by_category(\n",
    "    embedding_paths, \n",
    "    CLIP_EMBEDDINGS_DIR,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    use_parallel=USE_PARALLEL\n",
    ")\n",
    "\n",
    "# Compute category averages\n",
    "category_averages, categories = compute_category_averages(embeddings_by_category)\n",
    "\n",
    "# Save category averages\n",
    "output_dir = Path(CLIP_OUTPUT_DIR)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"\\nSaving category average embeddings...\")\n",
    "embeddings = np.array([category_averages[cat] for cat in categories])\n",
    "npz_path = output_dir / 'category_average_embeddings.npz'\n",
    "np.savez(npz_path, \n",
    "         embeddings=embeddings, \n",
    "         categories=np.array(categories))\n",
    "print(f\"  Saved to {npz_path}\")\n",
    "\n",
    "# Also save as CSV for easier inspection\n",
    "csv_path = output_dir / 'category_average_embeddings.csv'\n",
    "embeddings_df = pd.DataFrame(embeddings, index=categories)\n",
    "embeddings_df.to_csv(csv_path)\n",
    "print(f\"  Saved to {csv_path}\")\n",
    "\n",
    "# Save category info\n",
    "info_path = output_dir / 'category_average_info.txt'\n",
    "with open(info_path, 'w') as f:\n",
    "    f.write(f\"Total categories: {len(categories)}\\n\")\n",
    "    f.write(f\"Embedding dimension: {embeddings.shape[1]}\\n\")\n",
    "    f.write(f\"\\nCategories:\\n\")\n",
    "    for cat in categories:\n",
    "        f.write(f\"  {cat}: {len(embeddings_by_category[cat])} embeddings\\n\")\n",
    "print(f\"  Saved to {info_path}\")\n",
    "\n",
    "# Save category names\n",
    "names_path = output_dir / 'category_names.txt'\n",
    "with open(names_path, 'w') as f:\n",
    "    for cat in categories:\n",
    "        f.write(f\"{cat}\\n\")\n",
    "print(f\"  Saved to {names_path}\")\n",
    "\n",
    "print(f\"\\nCLIP average embeddings computation complete! Results saved to {output_dir}\")\n",
    "print(f\"Total categories: {len(categories)}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute DINOv3 Average Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COMPUTING DINOv3 AVERAGE EMBEDDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load embedding paths\n",
    "if MATCH_FROM_CLIP_LIST and CLIP_EMBEDDING_LIST_REF:\n",
    "    embedding_paths = match_embedding_paths_from_list(CLIP_EMBEDDING_LIST_REF, DINOV3_EMBEDDINGS_DIR)\n",
    "elif DINOV3_EMBEDDING_LIST:\n",
    "    embedding_paths = load_embedding_paths(DINOV3_EMBEDDING_LIST)\n",
    "else:\n",
    "    embedding_paths = scan_embedding_directory(DINOV3_EMBEDDINGS_DIR)\n",
    "\n",
    "# Load embeddings by category\n",
    "embeddings_by_category = load_embeddings_by_category(\n",
    "    embedding_paths, \n",
    "    DINOV3_EMBEDDINGS_DIR,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    use_parallel=USE_PARALLEL\n",
    ")\n",
    "\n",
    "# Compute category averages\n",
    "category_averages, categories = compute_category_averages(embeddings_by_category)\n",
    "\n",
    "# Save category averages\n",
    "output_dir = Path(DINOV3_OUTPUT_DIR)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"\\nSaving category average embeddings...\")\n",
    "embeddings = np.array([category_averages[cat] for cat in categories])\n",
    "npz_path = output_dir / 'category_average_embeddings.npz'\n",
    "np.savez(npz_path, \n",
    "         embeddings=embeddings, \n",
    "         categories=np.array(categories))\n",
    "print(f\"  Saved to {npz_path}\")\n",
    "\n",
    "# Also save as CSV for easier inspection\n",
    "csv_path = output_dir / 'category_average_embeddings.csv'\n",
    "embeddings_df = pd.DataFrame(embeddings, index=categories)\n",
    "embeddings_df.to_csv(csv_path)\n",
    "print(f\"  Saved to {csv_path}\")\n",
    "\n",
    "# Save category info\n",
    "info_path = output_dir / 'category_average_info.txt'\n",
    "with open(info_path, 'w') as f:\n",
    "    f.write(f\"Total categories: {len(categories)}\\n\")\n",
    "    f.write(f\"Embedding dimension: {embeddings.shape[1]}\\n\")\n",
    "    f.write(f\"\\nCategories:\\n\")\n",
    "    for cat in categories:\n",
    "        f.write(f\"  {cat}: {len(embeddings_by_category[cat])} embeddings\\n\")\n",
    "print(f\"  Saved to {info_path}\")\n",
    "\n",
    "# Save category names\n",
    "names_path = output_dir / 'category_names.txt'\n",
    "with open(names_path, 'w') as f:\n",
    "    for cat in categories:\n",
    "        f.write(f\"{cat}\\n\")\n",
    "print(f\"  Saved to {names_path}\")\n",
    "\n",
    "print(f\"\\nDINOv3 average embeddings computation complete! Results saved to {output_dir}\")\n",
    "print(f\"Total categories: {len(categories)}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vislearnlabpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
